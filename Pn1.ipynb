{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hondore16/hondore16/blob/main/Pn1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "42nZ4VXFXJ1R",
        "outputId": "95bb067c-1632-45b6-df0a-4ba7d2afca88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ GPU...\n",
            "âœ… GPU Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "\n",
            "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° (19683 ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸)...\n",
            "Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ·Ğ¼ĞµÑ€: (19683, 18)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ hidden_layer_1 (\u001b[38;5;33mDense\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m1,216\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ hidden_layer_2 (\u001b[38;5;33mDense\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ output_layer (\u001b[38;5;33mDense\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              â”‚            \u001b[38;5;34m66\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ hidden_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ hidden_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,362\u001b[0m (13.13 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,362</span> (13.13 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,362\u001b[0m (13.13 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,362</span> (13.13 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° GPU...\n",
            "Epoch 1/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.6974 - loss: 0.5048 - val_accuracy: 0.6602 - val_loss: 0.3062\n",
            "Epoch 2/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6824 - loss: 0.2581 - val_accuracy: 0.6714 - val_loss: 0.1926\n",
            "Epoch 3/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6761 - loss: 0.1794 - val_accuracy: 0.6805 - val_loss: 0.1547\n",
            "Epoch 4/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6744 - loss: 0.1477 - val_accuracy: 0.6851 - val_loss: 0.1270\n",
            "Epoch 5/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6696 - loss: 0.1210 - val_accuracy: 0.6709 - val_loss: 0.1056\n",
            "Epoch 6/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6766 - loss: 0.0966 - val_accuracy: 0.6541 - val_loss: 0.0890\n",
            "Epoch 7/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6697 - loss: 0.0776 - val_accuracy: 0.6755 - val_loss: 0.0723\n",
            "Epoch 8/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6683 - loss: 0.0622 - val_accuracy: 0.6338 - val_loss: 0.0623\n",
            "Epoch 9/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6714 - loss: 0.0492 - val_accuracy: 0.6379 - val_loss: 0.0544\n",
            "Epoch 10/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6722 - loss: 0.0424 - val_accuracy: 0.6552 - val_loss: 0.0437\n",
            "Epoch 11/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6802 - loss: 0.0321 - val_accuracy: 0.6943 - val_loss: 0.0369\n",
            "Epoch 12/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6877 - loss: 0.0260 - val_accuracy: 0.6643 - val_loss: 0.0282\n",
            "Epoch 13/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6892 - loss: 0.0198 - val_accuracy: 0.6821 - val_loss: 0.0228\n",
            "Epoch 14/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6853 - loss: 0.0156 - val_accuracy: 0.6719 - val_loss: 0.0180\n",
            "Epoch 15/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6822 - loss: 0.0120 - val_accuracy: 0.6658 - val_loss: 0.0142\n",
            "Epoch 16/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6847 - loss: 0.0097 - val_accuracy: 0.6811 - val_loss: 0.0110\n",
            "Epoch 17/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6882 - loss: 0.0076 - val_accuracy: 0.6831 - val_loss: 0.0088\n",
            "Epoch 18/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6920 - loss: 0.0057 - val_accuracy: 0.6755 - val_loss: 0.0069\n",
            "Epoch 19/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6900 - loss: 0.0047 - val_accuracy: 0.6816 - val_loss: 0.0050\n",
            "Epoch 20/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6882 - loss: 0.0036 - val_accuracy: 0.6760 - val_loss: 0.0050\n",
            "Epoch 21/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6895 - loss: 0.0029 - val_accuracy: 0.6851 - val_loss: 0.0035\n",
            "Epoch 22/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6932 - loss: 0.0022 - val_accuracy: 0.6821 - val_loss: 0.0029\n",
            "Epoch 23/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6981 - loss: 0.0018 - val_accuracy: 0.6902 - val_loss: 0.0022\n",
            "Epoch 24/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6966 - loss: 0.0015 - val_accuracy: 0.6836 - val_loss: 0.0020\n",
            "Epoch 25/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6962 - loss: 0.0012 - val_accuracy: 0.6892 - val_loss: 0.0016\n",
            "Epoch 26/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6949 - loss: 9.4257e-04 - val_accuracy: 0.6927 - val_loss: 0.0014\n",
            "Epoch 27/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6999 - loss: 7.9763e-04 - val_accuracy: 0.6877 - val_loss: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6995 - loss: 6.6620e-04 - val_accuracy: 0.6872 - val_loss: 8.9865e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6934 - loss: 5.6050e-04 - val_accuracy: 0.6739 - val_loss: 9.8396e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6967 - loss: 4.6030e-04 - val_accuracy: 0.6968 - val_loss: 6.2838e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6927 - loss: 3.7206e-04 - val_accuracy: 0.6892 - val_loss: 5.3219e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6970 - loss: 3.1805e-04 - val_accuracy: 0.6892 - val_loss: 5.0313e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6952 - loss: 2.6275e-04 - val_accuracy: 0.6856 - val_loss: 3.6136e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6941 - loss: 2.2518e-04 - val_accuracy: 0.6887 - val_loss: 3.3385e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6948 - loss: 1.7837e-04 - val_accuracy: 0.6866 - val_loss: 2.7891e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7019 - loss: 1.5628e-04 - val_accuracy: 0.6851 - val_loss: 2.3101e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6958 - loss: 1.3624e-04 - val_accuracy: 0.6887 - val_loss: 2.0142e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6946 - loss: 1.1145e-04 - val_accuracy: 0.6872 - val_loss: 1.7052e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6964 - loss: 9.3622e-05 - val_accuracy: 0.6902 - val_loss: 1.4046e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6995 - loss: 7.6869e-05 - val_accuracy: 0.6948 - val_loss: 1.1822e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6940 - loss: 7.0075e-05 - val_accuracy: 0.6887 - val_loss: 1.0658e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7020 - loss: 5.9864e-05 - val_accuracy: 0.6917 - val_loss: 9.9251e-05\n",
            "Epoch 43/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6993 - loss: 4.9872e-05 - val_accuracy: 0.6912 - val_loss: 7.4524e-05\n",
            "Epoch 44/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6956 - loss: 4.1560e-05 - val_accuracy: 0.6816 - val_loss: 7.7005e-05\n",
            "Epoch 45/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6980 - loss: 3.7587e-05 - val_accuracy: 0.6846 - val_loss: 7.3418e-05\n",
            "Epoch 46/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6949 - loss: 3.1611e-05 - val_accuracy: 0.6902 - val_loss: 4.5500e-05\n",
            "Epoch 47/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7001 - loss: 2.5268e-05 - val_accuracy: 0.6856 - val_loss: 4.9881e-05\n",
            "Epoch 48/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6955 - loss: 2.3244e-05 - val_accuracy: 0.6953 - val_loss: 3.4543e-05\n",
            "Epoch 49/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7030 - loss: 1.8094e-05 - val_accuracy: 0.6800 - val_loss: 3.9347e-05\n",
            "Epoch 50/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6930 - loss: 1.5484e-05 - val_accuracy: 0.6877 - val_loss: 2.9568e-05\n",
            "Epoch 51/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6985 - loss: 1.3355e-05 - val_accuracy: 0.6872 - val_loss: 2.3230e-05\n",
            "Epoch 52/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6976 - loss: 1.1565e-05 - val_accuracy: 0.6932 - val_loss: 2.0432e-05\n",
            "Epoch 53/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7010 - loss: 9.7609e-06 - val_accuracy: 0.6907 - val_loss: 1.5016e-05\n",
            "Epoch 54/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7000 - loss: 8.5012e-06 - val_accuracy: 0.6907 - val_loss: 1.3999e-05\n",
            "Epoch 55/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6978 - loss: 7.5579e-06 - val_accuracy: 0.6907 - val_loss: 1.2052e-05\n",
            "Epoch 56/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7041 - loss: 6.4089e-06 - val_accuracy: 0.6821 - val_loss: 1.0699e-05\n",
            "Epoch 57/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7000 - loss: 5.1883e-06 - val_accuracy: 0.6912 - val_loss: 1.1075e-05\n",
            "Epoch 58/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7010 - loss: 4.4251e-06 - val_accuracy: 0.6851 - val_loss: 8.8583e-06\n",
            "Epoch 59/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6966 - loss: 3.9316e-06 - val_accuracy: 0.6856 - val_loss: 7.8803e-06\n",
            "Epoch 60/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7050 - loss: 3.3091e-06 - val_accuracy: 0.6963 - val_loss: 6.2475e-06\n",
            "Epoch 61/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7041 - loss: 2.9550e-06 - val_accuracy: 0.6856 - val_loss: 5.0331e-06\n",
            "Epoch 62/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6994 - loss: 2.3304e-06 - val_accuracy: 0.6866 - val_loss: 4.4960e-06\n",
            "Epoch 63/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7005 - loss: 2.2940e-06 - val_accuracy: 0.6841 - val_loss: 4.0864e-06\n",
            "Epoch 64/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7015 - loss: 1.7523e-06 - val_accuracy: 0.6882 - val_loss: 3.4203e-06\n",
            "Epoch 65/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7005 - loss: 1.4295e-06 - val_accuracy: 0.6846 - val_loss: 2.5389e-06\n",
            "Epoch 66/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7023 - loss: 1.3221e-06 - val_accuracy: 0.6831 - val_loss: 2.8854e-06\n",
            "Epoch 67/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7032 - loss: 1.2591e-06 - val_accuracy: 0.6892 - val_loss: 2.1460e-06\n",
            "Epoch 68/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6948 - loss: 9.6546e-07 - val_accuracy: 0.6826 - val_loss: 2.3710e-06\n",
            "Epoch 69/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7041 - loss: 8.7295e-07 - val_accuracy: 0.6872 - val_loss: 1.6633e-06\n",
            "Epoch 70/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7020 - loss: 7.5292e-07 - val_accuracy: 0.6856 - val_loss: 1.2713e-06\n",
            "Epoch 71/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6910 - loss: 6.5139e-07 - val_accuracy: 0.6877 - val_loss: 1.2978e-06\n",
            "Epoch 72/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6958 - loss: 5.3319e-07 - val_accuracy: 0.6856 - val_loss: 1.1601e-06\n",
            "Epoch 73/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6909 - loss: 4.8632e-07 - val_accuracy: 0.6826 - val_loss: 1.3174e-06\n",
            "Epoch 74/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7012 - loss: 3.9732e-07 - val_accuracy: 0.6938 - val_loss: 8.4230e-07\n",
            "Epoch 75/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6947 - loss: 3.5934e-07 - val_accuracy: 0.6861 - val_loss: 7.4082e-07\n",
            "Epoch 76/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7053 - loss: 2.9868e-07 - val_accuracy: 0.6856 - val_loss: 6.2237e-07\n",
            "Epoch 77/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6949 - loss: 2.6356e-07 - val_accuracy: 0.6826 - val_loss: 6.7430e-07\n",
            "Epoch 78/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7027 - loss: 2.4254e-07 - val_accuracy: 0.6927 - val_loss: 4.4888e-07\n",
            "Epoch 79/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6964 - loss: 2.0652e-07 - val_accuracy: 0.6866 - val_loss: 4.6043e-07\n",
            "Epoch 80/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7028 - loss: 1.7548e-07 - val_accuracy: 0.6927 - val_loss: 4.1367e-07\n",
            "Epoch 81/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6982 - loss: 1.4896e-07 - val_accuracy: 0.6856 - val_loss: 3.5044e-07\n",
            "Epoch 82/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6973 - loss: 1.3020e-07 - val_accuracy: 0.6932 - val_loss: 4.0433e-07\n",
            "Epoch 83/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6994 - loss: 1.2111e-07 - val_accuracy: 0.6877 - val_loss: 2.4734e-07\n",
            "Epoch 84/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7014 - loss: 9.5124e-08 - val_accuracy: 0.6821 - val_loss: 2.6659e-07\n",
            "Epoch 85/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6971 - loss: 8.3099e-08 - val_accuracy: 0.6882 - val_loss: 2.4207e-07\n",
            "Epoch 86/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7029 - loss: 7.6005e-08 - val_accuracy: 0.6943 - val_loss: 2.1730e-07\n",
            "Epoch 87/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7027 - loss: 6.3253e-08 - val_accuracy: 0.6887 - val_loss: 1.4528e-07\n",
            "Epoch 88/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7000 - loss: 5.7363e-08 - val_accuracy: 0.6948 - val_loss: 1.3340e-07\n",
            "Epoch 89/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7007 - loss: 5.1684e-08 - val_accuracy: 0.6927 - val_loss: 1.3540e-07\n",
            "Epoch 90/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7006 - loss: 4.7532e-08 - val_accuracy: 0.6887 - val_loss: 1.0553e-07\n",
            "Epoch 91/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7054 - loss: 3.8306e-08 - val_accuracy: 0.6917 - val_loss: 9.5632e-08\n",
            "Epoch 92/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7027 - loss: 3.4415e-08 - val_accuracy: 0.6882 - val_loss: 1.1198e-07\n",
            "Epoch 93/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7051 - loss: 2.9972e-08 - val_accuracy: 0.6907 - val_loss: 8.5377e-08\n",
            "Epoch 94/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7023 - loss: 2.7186e-08 - val_accuracy: 0.6892 - val_loss: 7.3687e-08\n",
            "Epoch 95/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7024 - loss: 2.5123e-08 - val_accuracy: 0.6932 - val_loss: 6.5394e-08\n",
            "Epoch 96/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7039 - loss: 2.1241e-08 - val_accuracy: 0.6927 - val_loss: 5.2155e-08\n",
            "Epoch 97/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7029 - loss: 2.1663e-08 - val_accuracy: 0.6877 - val_loss: 5.3173e-08\n",
            "Epoch 98/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7037 - loss: 1.7966e-08 - val_accuracy: 0.6887 - val_loss: 6.4576e-08\n",
            "Epoch 99/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7030 - loss: 1.4968e-08 - val_accuracy: 0.6897 - val_loss: 3.8921e-08\n",
            "Epoch 100/100\n",
            "\u001b[1m277/277\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7079 - loss: 1.4387e-08 - val_accuracy: 0.6892 - val_loss: 4.6259e-08\n",
            "\n",
            "ğŸ† Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: 70.31%\n",
            "\n",
            "--- ĞšĞĞ” Ğ”Ğ›Ğ¯ Ğ’Ğ¡Ğ¢ĞĞ’ĞšĞ˜ Ğ’ VERILOG (ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒĞ¹ Ğ² weights.txt) ---\n",
            "// Layer 1 Weights (1152 values)\n",
            "localparam [15:0] L1_W [0:1151] = '{ 13, -40, 2, 48, 9, 2, 29, 33, -1, 7, -85, 14, 24, 23, 41, 12, -115, -12, 43, -1, 3, -59, -143, -4, 12, 2, 69, 0, -195, 30, -104, 30, 18, -70, 6, -76, -35, 80, -6, 35, -8, -9, -11, 29, -1, 23, 15, 6, -19, 80, 12, -8, 25, 12, 5, 57, 17, 36, 0, -36, -136, 7, 22, 10, 6, -42, -2, 31, 13, 7, 32, 2, -1, 4, 24, 5, 39, 7, -24, 46, -34, 11, -175, 3, 8, 25, 67, 8, 5, 22, -164, 8, -12, 2, 24, -99, 21, -64, 3, 5, 6, -10, 1, 90, -17, 7, -23, -22, 29, 62, -24, 3, -11, -70, 21, -1, 8, 36, -6, 12, -8, 15, 53, -11, 23, -9, -171, 4, -184, 107, 2, -23, 21, -35, 32, -64, -1, -6, 29, -21, 56, 12, 6, 4, -42, -20, 41, 2, 1, -45, -15, 7, -3, 18, 69, -4, -7, 68, 37, 43, 41, 48, 1, 4, 29, -23, 11, 23, -4, 40, -6, -18, 39, 52, -36, 13, 25, 91, 68, 2, 0, -29, 2, 5, -97, 55, 54, -18, 99, -14, -4, -101, -89, -8, -2, 28, 3, 5, 29, 3, 1, -1, -30, 1, 28, -6, -18, 46, 29, -122, -13, -2, 0, 79, -25, 6, -4, 9, -9, 1, 115, 43, 48, -60, -2, -11, -1, -2, 120, -36, 1, -9, 2, -14, -6, -19, 59, 25, 1, -2, 6, 20, 17, -11, -51, -14, 4, 8, 8, 48, 53, -5, -1, 31, -80, 67, -70, -40, 1, -8, 24, 65, 28, -2, 52, -15, 39, 6, 31, -21, -37, -28, -67, -30, 13, 5, -3, 85, 28, 3, 4, 51, -57, -29, -65, 35, 24, -184, 30, -23, 15, -48, 0, -3, -1, -58, 47, 69, -40, 72, 35, 53, -29, -4, 2, -21, 44, -6, 8, -11, -5, 6, 15, 19, 38, 28, -114, 19, -9, -206, 97, -188, 3, 24, 33, -2, -1, -33, 1, -3, -32, -2, 59, -3, 6, 32, -16, 84, -28, -1, 4, 16, -32, -2, 7, 7, 0, 3, -37, 61, 0, -37, 30, 36, -1, 4, -90, 54, -2, -8, 26, -23, -5, -14, 51, 63, -6, -2, -3, 29, 67, 2, -13, 29, 1, -67, -38, 73, 31, 13, 80, 47, 31, 36, 20, -77, 8, -33, -4, -48, 53, 49, 1, -3, -93, 9, 24, -24, -16, 8, 37, 87, -36, -5, 10, -76, -20, 15, 21, 12, -102, 5, 82, 9, 76, 65, -94, 21, -8, 8, -73, 28, 0, 29, 34, 51, 63, -25, 0, -9, 8, -5, -48, 58, -13, -7, -14, 27, -41, -5, 17, 59, 46, 12, -24, 21, -51, -208, -4, 3, -5, -3, 12, -103, 14, -32, 0, 9, -12, -3, 5, 8, 41, 69, 37, -5, 71, -1, 5, 13, -129, 4, 7, -1, -2, 3, -8, 4, 41, -153, 6, 30, -7, 6, 15, 7, 3, -83, 25, -15, 62, -16, 9, 0, 36, 4, -17, -57, 33, 6, 3, -35, 2, 16, 7, 62, 42, 50, 0, -16, 63, 66, 11, 74, 2, 16, 22, 102, 29, -3, -1, 11, 10, 0, 36, 8, 51, 84, 122, -17, -42, -14, 8, -32, 31, 9, 2, 4, -119, 2, -40, 63, -38, 94, 40, 49, 7, 2, 17, -58, 2, -9, 16, -32, 75, 21, 44, 46, 27, 4, -9, 59, 43, 5, 26, -33, 6, -99, 11, 51, 19, 58, -214, -70, -93, 46, 6, 0, 17, -36, 84, 4, -35, -23, 77, -1, 26, -86, -56, 14, 7, 44, 9, -1, 4, -18, 0, 32, 6, -1, -86, 16, -29, -49, 6, 10, 41, -4, -17, 3, -195, 83, 1, 4, 37, 3, 71, 46, 70, -54, 29, -9, -77, -69, 4, -26, 29, -52, -32, 9, -1, 18, -1, 39, 34, 65, 9, 25, 5, -2, 2, 3, -15, -13, -65, 8, -23, 35, 0, 36, -6, -44, -40, 34, -23, 52, 4, 2, -2, 44, -173, 17, 7, -12, 21, -60, 10, -2, 1, 23, -23, 2, -12, -20, 19, -3, 1, 24, 28, -9, 88, 39, 82, 12, 24, 14, 51, 99, 25, 37, 30, -9, -21, 16, -29, -10, -46, 29, -1, 25, 3, 36, 2, 2, 0, -4, 92, 36, -51, -5, 57, 82, 91, -23, 46, 81, -42, 48, 27, 27, 0, -4, -6, -15, 3, 41, -3, -94, -31, -9, -15, 13, -2, 2, -13, 4, -46, -24, -10, -12, -13, 13, -99, -2, 63, -44, 48, 46, 58, -50, -23, -82, -16, -19, 37, -17, 42, -47, 17, -4, 97, -1, 10, 29, -40, 68, 4, -2, 7, 3, -30, -64, 15, 9, -4, -6, -42, -85, -12, 3, 50, 72, 57, 60, 15, 34, -2, 45, -17, 0, 4, -168, 127, 30, -2, 47, -9, 5, -23, 1, -33, -3, 74, 3, -25, -9, 21, 16, -4, 36, 22, 64, 85, 52, -47, -3, 96, -8, 50, 107, 32, -22, 2, 67, -16, 59, 7, -23, -1, -2, 7, 0, 11, 8, 47, 42, -18, 4, -55, 28, -34, -39, -37, -15, 32, 53, 74, 51, -86, 12, 7, -160, 27, 7, 31, 29, 52, -77, 2, -7, -1, -14, -84, 9, -10, 21, -101, 92, 4, -11, -162, 41, -94, -54, 118, -22, 49, -3, 42, 5, 77, 57, 64, 56, -98, -7, -19, -24, 12, -10, 29, -67, 10, 17, 2, 11, -1, 2, -70, -11, -45, -3, 9, 52, -56, 42, 26, -180, 7, 35, 18, 45, -2, 0, -2, 36, -4, 8, -4, 90, 43, -13, 0, -95, -5, 2, -9, -6, -62, 17, -4, -29, -6, 2, -17, 1, 10, 38, 17, 14, 52, 26, -19, 11, -31, -1, 22, -170, 48, 44, 28, 6, 71, 15, -39, -14, -2, -13, 3, 0, 2, -1, 59, 82, 34, 5, -17, 3, 61, 72, 18, -140, 37, 48, 42, 10, -3, 6, 9, -63, -83, 36, -4, 17, -108, 67, -3, -16, -7, -59, -14, -33, 30, 9, 65, -64, 12, -8, -169, 16, 25, -52, -42, 54, 43, 25, -30, 14, 46, -12, 20, -59, -5, 11, -18, 9, -138, 48, 4, -11, -3, 16, 2, 0, 4, 5, -23, -11, 10, -5, -87, 8, 16, -1, -23, 14, 62, 65, 51, 42, 5, 3, 6, 71, 93, 11, 5, 8, 6, 66, 0, -26, 2, 18, 0, 3, -11, 4, -3, -50, -7, -18, 43, 66, -67, 30, -86, 45, 58, -18, -130, -181, 11, 30, 35, -19, -11, -9, 87, 8, -50, 19, -2, 27, 1, 67, 4, 13, -3, -3, -57, 4, 62, -1, 73, -27, 75, -111, -11, 29, 25, 18, -1, -14, -2, -11, 0, -67, -58, -2, 1, -55, 9, 37, -4, 38, 6, -23, 0, -20, 64, 7, -87, -167, -7, 36, 65, 11, -12, 46, -53, -17, -24, 5, 17, 21, -8, -17, 22, -4, -94, -38, -109, 31, -177, 30, 7, -30, -1, 15, 1, 1 };\n",
            "// Layer 1 Biases\n",
            "localparam [15:0] L1_B [0:63] = '{ 41, -1, 51, 23, 52, 41, 11, 2, 0, 3, 32, 39, 19, 16, 7, 23, 46, 42, 27, 11, 40, 64, 21, 26, 48, 44, 26, 35, -4, -7, 8, -17, 35, 52, 24, -29, 8, 39, 8, 49, 36, 45, 20, 21, -10, -10, 19, 37, 4, 10, -11, 5, -18, 44, 24, 26, -14, 12, 4, 26, 14, 13, 54, 27 };\n",
            "// Layer 2 Weights\n",
            "localparam [15:0] L2_W [0:2047] = '{ -19, -15, -42, 130, -162, -15, -46, -92, 141, 172, 30, 4, 30, 30, 166, 69, -29, 26, 106, 179, -91, 52, 85, -56, -28, 81, -31, 47, -2, -73, 36, 117, -63, -28, -47, 145, -223, 18, -61, -132, 212, 214, 86, 53, 77, 89, 184, 72, -64, 5, 179, 134, -163, -19, -104, -76, -39, 94, -45, 72, -43, -93, 2, 169, 58, -96, 85, -46, 10, -11, 86, 78, -17, 20, 116, 73, -26, 57, 22, 101, -72, -22, 69, 32, -45, -72, -23, 111, -79, -60, 90, 95, 96, -68, -81, -15, 24, -39, 61, -64, 18, -15, 83, 72, -55, -16, 51, 22, -27, 15, -13, 13, 5, -23, -16, 15, 52, -59, -29, 42, -30, -12, 37, 54, 19, -5, -36, -54, 77, -38, 64, -6, 21, -165, 79, 85, -41, 10, 68, 11, -14, 37, 36, 40, -17, -33, 0, 14, 5, -131, 3, 87, -35, -25, 29, 36, 65, 13, -6, -34, -16, -1, 16, 97, -44, 2, -5, -38, 87, 118, 52, 26, 36, 57, 112, 25, -2, 7, 70, 115, -95, -76, -216, -38, -26, 31, -28, 76, -13, -40, 13, 67, 13, -7, 56, -2, 63, 63, 62, 58, -12, -8, 36, 36, -18, 15, -37, -5, -14, -24, 10, -47, 14, 25, 78, 81, 5, -59, 33, 20, 50, 9, -52, -59, -80, 59, -58, 32, -31, 55, -28, -12, 56, 67, -29, -24, 72, -4, 18, -34, 59, -22, 36, 46, -32, 2, -13, -25, 21, 78, 19, -10, -42, -8, 18, 51, -15, 78, -30, 76, 3, 37, -74, -11, 37, 34, -49, -134, 82, -48, -19, -52, 51, -31, -3, 23, 62, 82, -119, -39, 59, 95, -5, -79, -86, 30, 40, 68, 24, -91, 74, -14, 2, 77, 24, 54, 11, 11, 60, 156, -20, 22, 54, 58, -32, -29, 3, 5, -9, -8, -7, 36, -63, -60, 95, 43, 68, -20, -116, -21, -2, 7, 12, 26, -15, 57, 25, -33, 69, 61, 14, 7, 18, 44, 33, 40, -15, -5, 93, 57, -13, 60, -127, -27, -31, 26, 6, 52, 19, -43, -17, 53, 163, -180, 164, -61, 22, -128, 145, 92, -38, -22, 158, 230, -116, 106, 41, 116, -152, 19, 67, 50, -38, -55, 21, 153, -165, -89, 176, 121, 199, -70, -168, -51, 16, 59, 16, -2, 47, -74, -7, 3, -6, -44, -29, -3, 31, -30, -24, -13, 29, -27, -49, -29, 76, 3, -17, 11, 68, 27, -32, 1, 17, 30, 29, 35, -38, 78, 0, 30, 26, 18, -10, -15, 0, 15, -55, -33, 39, -32, 19, -1, 85, -15, -15, 0, 21, 1, -6, -5, 58, 53, 16, -60, -11, 68, 71, 59, -6, 41, -5, 29, 25, -79, -11, -21, -30, -9, -54, 3, 52, -32, -32, -48, 28, -9, -10, -51, 30, 15, 67, -29, 45, 8, 29, 4, -9, 60, 53, 44, 1, 45, -3, 37, 63, 28, 19, 35, -23, -39, -17, -17, 39, -34, -17, -30, 45, -25, -31, -4, 37, 48, 54, 8, 27, 36, 27, -24, 5, 36, 61, 30, 22, -7, -21, 77, -80, 8, -10, -34, 73, 122, 52, 44, 37, 90, 114, 35, -20, -24, 121, 141, -61, 119, 217, -48, -46, 64, -33, 53, -33, -51, 24, 48, 0, 37, -17, 77, -102, -53, -61, -66, 127, 80, 45, 14, 68, 57, 84, 17, 6, 9, 81, 117, -97, 81, 101, -55, 1, 69, -33, 19, -26, -39, -1, 65, -1, 18, 3, 80, -129, -72, -58, -83, 114, 102, 47, 28, 69, 29, 97, 46, -38, -1, 118, 81, -98, 4, 37, -5, -53, 47, 2, 21, 22, -76, -30, 101, 134, -109, 120, -60, 32, 31, 102, 95, -15, 37, 124, 102, -66, 96, 20, 119, -85, 24, 21, 45, -54, -41, -35, 58, -121, -37, 37, 133, 127, -61, -95, -17, 150, -89, 105, -53, 29, -154, 90, 101, -7, -3, 144, 167, -51, 115, 72, 101, -112, -23, 89, 39, -10, -1, 7, 118, -141, -97, 130, 115, 139, -68, -122, -33, 3, 32, -26, 68, -19, 39, -32, -14, 84, 55, 27, 11, 69, 47, 64, 34, -18, 4, 59, 65, 5, -70, -140, -34, 41, 79, 28, 37, -10, -16, 43, 51, -22, -20, -43, 73, -72, -36, -16, -36, 107, 135, 51, 53, 62, 19, 119, 52, 4, -13, 85, 115, -77, -18, -131, -13, -9, 52, 5, 26, -10, -33, 10, 75, 145, -120, 129, -67, 19, -144, 107, 77, -37, -26, 121, 159, -79, 64, 45, 117, -82, 21, 21, 20, -35, -1, 3, 86, -125, -51, 160, 83, 114, -92, -144, -38, 52, -53, 98, -36, 9, -132, 99, 94, -47, -12, 103, 93, -57, 60, 30, 97, -27, -9, 34, -28, -6, 59, -13, 108, -64, -42, 45, 77, 84, -22, -54, -15, 58, -50, 65, -49, 23, -142, 50, 72, -22, -7, 44, 83, -61, 5, 18, 42, -44, -9, 32, -9, 44, -98, -55, 19, -33, -16, 44, 61, 31, 22, -6, 8, -22, -32, -90, 142, -172, -3, -66, -119, 220, 247, 47, 42, 99, 91, 152, 84, -59, 18, 187, 250, -136, 28, -43, -55, -59, 120, -30, 91, -1, -100, 33, 169, 62, -90, 52, 3, 3, -43, 70, 9, 25, 1, 50, 95, -61, 27, 55, 82, -82, -24, 79, 10, -23, 39, 45, 43, -51, -20, 92, 88, 102, -26, -35, -23, -67, 15, -31, 146, -214, -23, -82, -114, 196, 187, 70, 41, 64, 66, 150, 75, -46, 4, 154, 224, -129, 1, -33, -36, -46, 131, -73, 73, -29, -73, 7, 109, -12, 17, 40, 4, 36, 47, 43, 51, -56, -66, -11, 31, 0, -47, -56, 5, 40, -28, -24, -73, 32, -34, -37, 27, 63, 6, -29, -36, -22, 22, -2, -37, 75, -5, 44, -41, 104, 24, 51, 83, -70, -46, 11, 14, -61, -12, -43, -28, 25, 4, -30, -29, 63, 20, 46, 47, 30, -66, 17, 2, 52, 62, -29, -57, -24, -21, -32, 117, -76, 1, -54, -88, 143, 99, 50, 60, 61, 117, 108, 46, -54, 26, 124, -34, -109, -71, -37, -47, -18, 102, -41, 99, -4, -65, -7, 121, 35, -15, 26, -3, 11, -72, 54, 36, 11, -9, 52, 44, -22, 57, 3, 17, -22, 29, 4, 49, 39, -59, -125, 58, -6, -41, 47, 32, 19, 3, -22, -32, 33, -30, 42, 11, -51, -61, -4, -23, 76, 64, 5, 14, 8, 57, 69, 37, 14, 17, 53, 49, -49, 77, 97, 7, -39, 20, -11, 4, 5, -21, -9, 61, 196, -147, 171, -109, -14, -140, 121, 98, -47, 0, 188, 176, -139, 90, 39, 159, -123, -5, 77, 52, -72, -22, -8, 78, -163, -120, 126, 132, 178, -131, -190, -97, 171, -105, 115, -139, 11, 18, 93, 80, -69, -47, 97, 141, -139, 87, 13, 115, -90, 22, 62, 35, -49, -29, 118, 136, -106, -99, 134, 123, 163, -83, -134, -109, -36, -28, -14, 49, -82, -12, 9, -44, 99, 104, 35, 50, 25, 33, 104, 48, -24, 5, 115, 120, -55, 8, 14, -6, -46, 33, 0, 49, 34, -23, -21, 29, -21, 25, -23, 32, -9, 95, -13, -34, 61, 71, 21, 42, 7, 47, 90, 57, -24, -23, 33, 68, 15, -19, -115, 38, -9, 30, 1, 41, -7, 0, 18, 41, 167, -153, 152, -87, 25, -19, 82, 121, -40, 5, 146, 176, -122, 109, 46, 135, -149, -27, 18, 6, -20, -65, -58, 122, -123, -88, -2, 124, 198, -102, -136, -41, 10, 28, -14, 82, -49, -97, -58, -25, 91, 56, 14, -22, 62, 33, 42, -7, 6, -9, 80, 61, -36, -38, -142, -40, 12, 73, -20, 13, -48, 21, 26, 62, 81, -70, 70, -31, 68, 119, 88, 45, -26, -54, 42, 54, -32, 48, 9, 62, -22, 14, -10, -40, 29, -18, 11, 61, -12, -79, 48, 57, 80, -10, -44, -59, -26, 42, -12, 80, -25, 53, -27, -23, 66, 57, 6, 29, 66, 18, 39, 3, 23, -15, 5, 20, 30, -10, -81, -12, 22, 61, -137, -15, -48, 23, 58, 23, -112, 83, -85, 83, -48, 56, -33, -43, 81, 67, -33, -33, 97, -44, -7, -22, 89, -24, 6, 12, 50, 41, 139, -60, 84, 103, -49, -36, -55, 40, 79, 74, -34, 71, -50, 41, -2, 1, -32, -60, 78, 54, -19, -39, 51, -20, 33, -29, 48, 7, 22, 7, 22, -10, -53, -76, 56, 90, -30, -41, -11, 48, 26, 77, -21, 71, -35, 9, 41, 16, -29, 43, -38, -19, -23, -29, 34, -41, -56, -65, 28, -27, -39, -8, 70, -29, -22, -29, 48, 26, 0, -53, -41, 66, 69, 12, 46, 50, 2, 11, 44, 26, 10, 12, -46, -47, -43, -11, -14, -35, -33, -24, 40, -6, -59, -7, 21, -31, -54, 8, 56, -3, -46, -27, 15, 47, 3, -13, 6, -52, 37, -33, -6, -53, 68, 35, -17, -11, 32, 98, -43, -4, 21, 44, -25, -3, -19, -24, -16, -11, 122, 55, -67, -53, 52, 50, 52, -42, -26, -16, 132, -136, 117, -87, 11, -87, 112, 66, -42, -7, 120, 179, -81, 120, 53, 141, -133, 29, 94, 43, -30, -36, 10, 157, -129, -107, 155, 144, 136, -78, -110, -70, -13, 53, -40, 24, -17, -25, -23, -53, 12, 27, -6, -37, 16, 11, -8, -31, 73, 0, 29, 21, 43, 35, -80, -5, 52, 17, -32, -7, -44, 59, 25, 67, 56, -1, 51, -84, 65, -7, 61, 83, -63, -66, 1, 44, -23, -40, -57, 12, 10, -4, -77, -121, 95, 20, -17, 60, 47, -51, 66, -27, 15, 61, -14, -81, -34, 41, -17, 0, 61, 3, 25, 11, -17, -26, -55, -24, 8, -41, -56, -32, 70, 27, -28, -31, 76, 2, -45, 2, 72, -11, -17, -40, 7, 52, 65, 3, 119, -118, 121, -80, 23, 14, 80, 36, 9, -8, 101, 169, -83, 94, 37, 136, -113, 2, 37, 12, -37, -23, 24, 120, -99, -80, 170, 112, 124, -79, -164, -67, 86, -65, 117, -88, 19, -67, 81, 98, -41, -51, 62, 91, -57, 38, -3, 39, -44, -17, -42, -109, -27, -86, -43, 51, -73, -46, 112, 103, 109, -24, -108, -67, 0, -48, 6, 12, -15, 30, 41, 13, 11, 52, 18, 50, 0, 57, 57, 23, -46, -5, 65, 53, 17, -4, -88, 22, -43, -15, -24, 16, 56, -14, -33, 24, 78, -91, 78, -23, -21, -63, 61, 69, -11, 32, 94, 101, -71, 54, 33, 45, -58, 17, 31, 7, -42, -47, -28, 71, -94, -24, 74, 85, 82, -56, -69, -50, 16, 16, 2, 73, -36, -28, -7, -16, 76, 60, -18, -20, 50, 36, 20, 30, 35, -14, 21, 72, -38, 5, -116, 0, 36, 68, -98, -22, 15, -32, 20, 37, 17, -60, 56, -20, 41, -107, 39, 92, -15, -36, 100, 88, -94, 40, 70, 54, -80, -30, 28, 42, -35, -24, 94, 60, -87, -79, 110, 103, 81, -23, -55, -68, 36, 21, 11, -25, 56, 4, 41, 19, 0, -33, -23, -41, 20, -7, -36, -22, 57, -16, -49, -32, 43, -15, 31, 37, 51, 19, 9, -9, -7, 54, 3, 24, 13, 36, 25, -25, 48, 4, 30, 15, -34, -29, -16, -10, -1, 16, -32, -3, 41, -14, -44, -21, 37, -38, -11, 28, 22, -4, 52, -26, 23, 38, 22, -23, 73, -56, 32, -49, 63, 46, 35, 42, -42, -53, 5, 55, -16, -8, -5, 17, 19, 10, -4, -27, 45, 87, 96, 75, 30, -60, -10, 25, 66, 23, -19, -51, -36, -28, -51, 136, -241, -1, -57, -111, 241, 233, 73, 46, 113, 72, 248, 59, -21, -22, 208, 263, -226, -24, -114, -90, -62, 122, -34, 74, -20, -97, 9, 115, -15, 31, -15, 11, 21, 7, -47, 22, 8, -15, -37, -20, 55, -19, 0, -29, 32, 13, 29, 22, 12, 11, -160, -15, 59, 30, 16, 0, -23, 31, 30, 27, -29, -26, -64, 128, -121, 6, -16, -50, 134, 164, 60, 10, 61, 54, 120, 56, -31, 2, 153, 194, -132, 9, -50, -63, -13, 90, -14, 36, -22, -78, -29, 132, -11, -28, -32, 148, -248, 0, -67, -86, 212, 248, 68, 35, 72, 107, 210, 60, -4, -6, 200, 255, -163, 23, -156, -16, -11, 57, -95, 69, -29, -104, 9, 129 };\n",
            "// Layer 2 Biases\n",
            "localparam [15:0] L2_B [0:31] = '{ 18, -2, 18, 14, 9, 17, 10, 16, 15, 12, 12, 39, 3, 14, 32, 28, 6, -3, 29, 22, 14, 23, -5, 15, 3, 18, 16, 14, 17, 8, -1, 17 };\n",
            "// Output Layer Weights\n",
            "localparam [15:0] L3_W [0:63] = '{ 70, -208, 28, 157, 87, -224, -285, 137, 249, -14, 38, 323, 113, -186, 161, -158, -286, 49, -301, 23, -72, -187, -42, -282, -86, 105, -158, -165, -318, -73, -138, -273, 70, 143, 44, 32, -311, -61, -324, -19, 198, 70, 237, 162, 465, 5, 97, -192, 75, 151, -140, 110, 148, -280, -83, -166, 51, -264, 141, 114, 3, 197, -184, 94 };\n",
            "// Output Layer Biases\n",
            "localparam [15:0] L3_B [0:1] = '{ -4, -7 };\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# --- 1. ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ GPU ---\n",
        "print(\"ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ GPU...\")\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"âœ… GPU Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {gpus[0]}\")\n",
        "else:\n",
        "    print(\"âš ï¸ GPU Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡Ğ¸Ğ»Ğ¸ ĞµĞ³Ğ¾ Ğ² 'Ğ¡Ñ€ĞµĞ´Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ' -> 'Ğ¡Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ ÑÑ€ĞµĞ´Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ'.\")\n",
        "\n",
        "# --- 2. Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ¯ Ğ”ĞĞĞĞ«Ğ¥ ---\n",
        "def check_winner(board):\n",
        "    grid = np.array(board).reshape(3, 3)\n",
        "    win_x = 0\n",
        "    win_o = 0\n",
        "\n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ñ€Ğ¾Ğº, ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ĞµĞ¹\n",
        "    lines = []\n",
        "    for i in range(3):\n",
        "        lines.append(grid[i, :])\n",
        "        lines.append(grid[:, i])\n",
        "    lines.append(grid.diagonal())\n",
        "    lines.append(np.fliplr(grid).diagonal())\n",
        "\n",
        "    for line in lines:\n",
        "        if np.all(line == 1): win_x = 1\n",
        "        if np.all(line == 2): win_o = 1\n",
        "    return np.array([win_x, win_o])\n",
        "\n",
        "print(\"\\nĞ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° (19683 ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸)...\")\n",
        "all_states = list(itertools.product([0, 1, 2], repeat=9))\n",
        "\n",
        "X_data = []\n",
        "Y_data = []\n",
        "\n",
        "for state in all_states:\n",
        "    # Ğ’Ñ…Ğ¾Ğ´: 18 Ğ±Ğ¸Ñ‚ (9 Ğ±Ğ¸Ñ‚ ĞºĞ°Ñ€Ñ‚Ğ° X, 9 Ğ±Ğ¸Ñ‚ ĞºĞ°Ñ€Ñ‚Ğ° O)\n",
        "    layer_x = [1 if cell == 1 else 0 for cell in state]\n",
        "    layer_o = [1 if cell == 2 else 0 for cell in state]\n",
        "    input_vector = layer_x + layer_o\n",
        "\n",
        "    winner = check_winner(state)\n",
        "\n",
        "    X_data.append(input_vector)\n",
        "    Y_data.append(winner)\n",
        "\n",
        "X_data = np.array(X_data)\n",
        "Y_data = np.array(Y_data)\n",
        "\n",
        "# ĞŸĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ\n",
        "indices = np.arange(X_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X_data = X_data[indices]\n",
        "Y_data = Y_data[indices]\n",
        "\n",
        "print(f\"Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {X_data.shape}\")\n",
        "\n",
        "# --- 3. Ğ¡ĞĞ—Ğ”ĞĞĞ˜Ğ• ĞœĞĞ©ĞĞĞ™ ĞœĞĞ”Ğ•Ğ›Ğ˜ ---\n",
        "model = Sequential()\n",
        "\n",
        "# Ğ¡Ğ»Ğ¾Ğ¹ 1: Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ»Ğ¸ Ğ´Ğ¾ 64 Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² (Ğ±Ñ‹Ğ»Ğ¾ 16)\n",
        "model.add(Dense(64, input_dim=18, activation='relu', name='hidden_layer_1'))\n",
        "\n",
        "# Ğ¡Ğ»Ğ¾Ğ¹ 2: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞµÑ‰Ğµ Ğ¾Ğ´Ğ¸Ğ½ ÑĞ»Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸\n",
        "model.add(Dense(32, activation='relu', name='hidden_layer_2'))\n",
        "\n",
        "# Ğ’Ñ‹Ñ…Ğ¾Ğ´: 2 Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ° (ĞŸĞ¾Ğ±ĞµĞ´Ğ° X, ĞŸĞ¾Ğ±ĞµĞ´Ğ° O)\n",
        "model.add(Dense(2, activation='sigmoid', name='output_layer'))\n",
        "\n",
        "# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Adam Ñ Ñ‡ÑƒÑ‚ÑŒ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# --- 4. ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• ---\n",
        "print(\"\\nĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° GPU...\")\n",
        "# Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¿Ğ¾Ñ… Ğ´Ğ¾ 100\n",
        "history = model.fit(X_data, Y_data, epochs=100, batch_size=64, verbose=1, validation_split=0.1)\n",
        "\n",
        "# Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°\n",
        "loss, acc = model.evaluate(X_data, Y_data, verbose=0)\n",
        "print(f\"\\nğŸ† Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: {acc*100:.2f}%\")\n",
        "\n",
        "# --- 5. Ğ­ĞšĞ¡ĞŸĞĞ Ğ¢ Ğ’Ğ•Ğ¡ĞĞ’ (FIXED POINT) ---\n",
        "# Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ñƒ Ğ½Ğ°Ñ 3 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ²ĞµÑĞ¾Ğ² (Ğ’Ñ…Ğ¾Ğ´->Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹1, Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹1->Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹2, Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹2->Ğ’Ñ‹Ñ…Ğ¾Ğ´)\n",
        "# ĞĞ¾ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ RTL (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ñ‚Ñ€Ğ¸ ÑĞ»Ğ¾Ñ Ğ½Ğ° Verilog),\n",
        "# Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ 1-Ğ³Ğ¾ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°,\n",
        "# ĞµÑĞ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ°Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚.\n",
        "# ĞĞ´Ğ½Ğ°ĞºĞ¾, Ñ€Ğ°Ğ· Ğ¼Ñ‹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ğ¹, Ğ½ÑƒĞ¶Ğ½Ğ¾ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ñ‚Ñ€Ğ¸.\n",
        "\n",
        "SCALE = 128\n",
        "\n",
        "def to_fixed(weights):\n",
        "    return [int(round(w * SCALE)) for w in weights.flatten()]\n",
        "\n",
        "print(\"\\n--- ĞšĞĞ” Ğ”Ğ›Ğ¯ Ğ’Ğ¡Ğ¢ĞĞ’ĞšĞ˜ Ğ’ VERILOG (ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒĞ¹ Ğ² weights.txt) ---\")\n",
        "\n",
        "# Ğ¡Ğ»Ğ¾Ğ¹ 1\n",
        "w1, b1 = model.get_layer('hidden_layer_1').get_weights()\n",
        "print(f\"// Layer 1 Weights ({len(to_fixed(w1))} values)\")\n",
        "print(f\"localparam [15:0] L1_W [0:{len(to_fixed(w1))-1}] = '{{ {', '.join(map(str, to_fixed(w1)))} }};\")\n",
        "print(f\"// Layer 1 Biases\")\n",
        "print(f\"localparam [15:0] L1_B [0:{len(to_fixed(b1))-1}] = '{{ {', '.join(map(str, to_fixed(b1)))} }};\")\n",
        "\n",
        "# Ğ¡Ğ»Ğ¾Ğ¹ 2\n",
        "w2, b2 = model.get_layer('hidden_layer_2').get_weights()\n",
        "print(f\"// Layer 2 Weights\")\n",
        "print(f\"localparam [15:0] L2_W [0:{len(to_fixed(w2))-1}] = '{{ {', '.join(map(str, to_fixed(w2)))} }};\")\n",
        "print(f\"// Layer 2 Biases\")\n",
        "print(f\"localparam [15:0] L2_B [0:{len(to_fixed(b2))-1}] = '{{ {', '.join(map(str, to_fixed(b2)))} }};\")\n",
        "\n",
        "# Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¹\n",
        "w3, b3 = model.get_layer('output_layer').get_weights()\n",
        "print(f\"// Output Layer Weights\")\n",
        "print(f\"localparam [15:0] L3_W [0:{len(to_fixed(w3))-1}] = '{{ {', '.join(map(str, to_fixed(w3)))} }};\")\n",
        "print(f\"// Output Layer Biases\")\n",
        "print(f\"localparam [15:0] L3_B [0:{len(to_fixed(b3))-1}] = '{{ {', '.join(map(str, to_fixed(b3)))} }};\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# --- 1. ĞĞĞ¡Ğ¢Ğ ĞĞ™ĞšĞ GPU ---\n",
        "print(\"ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° GPU...\")\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"âœ… GPU Ğ°ĞºÑ‚Ğ¸Ğ²ĞµĞ½! ({gpus[0]})\")\n",
        "else:\n",
        "    print(\"âš ï¸ GPU Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½ĞµĞµ, Ğ½Ğ¾ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚.\")\n",
        "\n",
        "# --- 2. Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ¯ Ğ˜ Ğ§Ğ˜Ğ¡Ğ¢ĞšĞ Ğ”ĞĞĞĞ«Ğ¥ ---\n",
        "def check_winner(board):\n",
        "    grid = np.array(board).reshape(3, 3)\n",
        "    win_x = 0\n",
        "    win_o = 0\n",
        "\n",
        "    lines = []\n",
        "    for i in range(3):\n",
        "        lines.append(grid[i, :])       # Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ¸\n",
        "        lines.append(grid[:, i])       # Ğ¡Ñ‚Ğ¾Ğ»Ğ±Ñ†Ñ‹\n",
        "    lines.append(grid.diagonal())      # Ğ”Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒ 1\n",
        "    lines.append(np.fliplr(grid).diagonal()) # Ğ”Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒ 2\n",
        "\n",
        "    for line in lines:\n",
        "        if np.all(line == 1): win_x = 1\n",
        "        if np.all(line == 2): win_o = 1\n",
        "    return win_x, win_o\n",
        "\n",
        "print(\"\\nĞ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²ÑĞµÑ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (19683)...\")\n",
        "all_states = list(itertools.product([0, 1, 2], repeat=9))\n",
        "\n",
        "X_raw = []\n",
        "Y_raw = []\n",
        "\n",
        "# Ğ¡Ñ‡ĞµÑ‚Ñ‡Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸\n",
        "count_x_wins = 0\n",
        "count_o_wins = 0\n",
        "count_draws = 0\n",
        "\n",
        "for state in all_states:\n",
        "    wx, wo = check_winner(state)\n",
        "\n",
        "    # --- Ğ¤Ğ˜Ğ›Ğ¬Ğ¢Ğ ĞĞ¦Ğ˜Ğ¯ ---\n",
        "    # Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ğ»Ğ¸ Ğ¾Ğ±Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ - ÑÑ‚Ğ¾ Ğ¼ÑƒÑĞ¾Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾Ğ½Ğ¸ Ğ¿ÑƒÑ‚Ğ°ÑÑ‚ ÑĞµÑ‚ÑŒ.\n",
        "    # Ğ’ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ğµ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚. Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ¸Ñ….\n",
        "    if wx == 1 and wo == 1:\n",
        "        continue\n",
        "\n",
        "    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²Ñ…Ğ¾Ğ´ (18 Ğ±Ğ¸Ñ‚)\n",
        "    layer_x = [1 if cell == 1 else 0 for cell in state]\n",
        "    layer_o = [1 if cell == 2 else 0 for cell in state]\n",
        "\n",
        "    X_raw.append(layer_x + layer_o)\n",
        "    Y_raw.append([wx, wo])\n",
        "\n",
        "    if wx: count_x_wins += 1\n",
        "    elif wo: count_o_wins += 1\n",
        "    else: count_draws += 1\n",
        "\n",
        "X_data = np.array(X_raw)\n",
        "Y_data = np.array(Y_raw)\n",
        "\n",
        "print(f\"Ğ§Ğ¸ÑÑ‚Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹: {len(X_data)} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ².\")\n",
        "print(f\"Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°: ĞŸĞ¾Ğ±ĞµĞ´ X: {count_x_wins}, ĞŸĞ¾Ğ±ĞµĞ´ O: {count_o_wins}, ĞĞ¸Ñ‡ÑŒĞ¸Ñ…/ĞŸÑƒÑÑ‚Ğ¾: {count_draws}\")\n",
        "\n",
        "# ĞŸĞ•Ğ Ğ•ĞœĞ•Ğ¨Ğ˜Ğ’ĞĞĞ˜Ğ• (ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾!)\n",
        "indices = np.arange(len(X_data))\n",
        "np.random.shuffle(indices)\n",
        "X_data = X_data[indices]\n",
        "Y_data = Y_data[indices]\n",
        "\n",
        "# --- 3. ĞĞ Ğ¥Ğ˜Ğ¢Ğ•ĞšĞ¢Ğ£Ğ Ğ \"Ğ¢Ğ¯Ğ–Ğ•Ğ›ĞĞ’Ğ•Ğ¡\" ---\n",
        "model = Sequential()\n",
        "\n",
        "# Ğ¡Ğ»Ğ¾Ğ¹ 1: 128 Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ½Ğ°Ğ²ĞµÑ€Ğ½ÑĞºĞ°)\n",
        "model.add(Dense(128, input_dim=18, activation='relu', name='layer1'))\n",
        "\n",
        "# Ğ¡Ğ»Ğ¾Ğ¹ 2: 64 Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ° (Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ)\n",
        "model.add(Dense(64, activation='relu', name='layer2'))\n",
        "\n",
        "# Ğ’Ñ‹Ñ…Ğ¾Ğ´: 2 Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°\n",
        "model.add(Dense(2, activation='sigmoid', name='output'))\n",
        "\n",
        "# Learning rate 0.001 - Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ°Ñ ÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ğ°\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# --- 4. ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• ---\n",
        "print(\"\\nĞ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Epochs=150)...\")\n",
        "# class_weight Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ±ĞµĞ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ğ½Ğ¸Ñ‡ÑŒĞ¸Ñ…\n",
        "# ĞĞ¾ Ñ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞµÑ‚ÑŒÑ Ğ¾Ğ½Ğ° ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑÑ Ğ¸ Ñ‚Ğ°Ğº.\n",
        "history = model.fit(\n",
        "    X_data, Y_data,\n",
        "    epochs=150,          # Ğ”Ğ°ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸\n",
        "    batch_size=128,      # ĞŸĞ°ĞºĞµÑ‚Ñ‹ Ğ¿Ğ¾Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°\n",
        "    verbose=1,\n",
        "    validation_split=0.1 # 10% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸\n",
        ")\n",
        "\n",
        "# Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°\n",
        "loss, acc = model.evaluate(X_data, Y_data, verbose=0)\n",
        "print(f\"\\nğŸ”¥ğŸ”¥ Ğ˜Ğ¢ĞĞ“ĞĞ’ĞĞ¯ Ğ¢ĞĞ§ĞĞĞ¡Ğ¢Ğ¬: {acc*100:.2f}% ğŸ”¥ğŸ”¥\")\n",
        "\n",
        "if acc < 0.98:\n",
        "    print(\"Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ½Ğ¾... ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ĞµÑ‰Ğµ Ñ€Ğ°Ğ·, Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ğ°Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼ĞµÑˆĞ°ĞµÑ‚.\")\n",
        "else:\n",
        "    print(\"ĞÑ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚! ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑĞ°.\")\n",
        "\n",
        "# --- 5. Ğ­ĞšĞ¡ĞŸĞĞ Ğ¢ Ğ’Ğ•Ğ¡ĞĞ’ ---\n",
        "SCALE = 128\n",
        "def to_fixed(weights):\n",
        "    return [int(round(w * SCALE)) for w in weights.flatten()]\n",
        "\n",
        "print(\"\\n--- ĞšĞĞ” Ğ”Ğ›Ğ¯ Ğ’Ğ¡Ğ¢ĞĞ’ĞšĞ˜ Ğ’ VERILOG (Ğ¤Ğ°Ğ¹Ğ» weights.txt) ---\")\n",
        "\n",
        "# Layer 1\n",
        "w1, b1 = model.get_layer('layer1').get_weights()\n",
        "print(f\"localparam [15:0] L1_W [0:{len(to_fixed(w1))-1}] = '{{ {', '.join(map(str, to_fixed(w1)))} }};\")\n",
        "print(f\"localparam [15:0] L1_B [0:{len(to_fixed(b1))-1}] = '{{ {', '.join(map(str, to_fixed(b1)))} }};\")\n",
        "\n",
        "# Layer 2\n",
        "w2, b2 = model.get_layer('layer2').get_weights()\n",
        "print(f\"localparam [15:0] L2_W [0:{len(to_fixed(w2))-1}] = '{{ {', '.join(map(str, to_fixed(w2)))} }};\")\n",
        "print(f\"localparam [15:0] L2_B [0:{len(to_fixed(b2))-1}] = '{{ {', '.join(map(str, to_fixed(b2)))} }};\")\n",
        "\n",
        "# Output\n",
        "w3, b3 = model.get_layer('output').get_weights()\n",
        "print(f\"localparam [15:0] L3_W [0:{len(to_fixed(w3))-1}] = '{{ {', '.join(map(str, to_fixed(w3)))} }};\")\n",
        "print(f\"localparam [15:0] L3_B [0:{len(to_fixed(b3))-1}] = '{{ {', '.join(map(str, to_fixed(b3)))} }};\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UAv4F69DYoAo",
        "outputId": "b606c140-877a-478e-84cb-003b5db66579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° GPU...\n",
            "âœ… GPU Ğ°ĞºÑ‚Ğ¸Ğ²ĞµĞ½! (PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'))\n",
            "\n",
            "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²ÑĞµÑ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (19683)...\n",
            "Ğ§Ğ¸ÑÑ‚Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹: 19371 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ².\n",
            "Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°: ĞŸĞ¾Ğ±ĞµĞ´ X: 4123, ĞŸĞ¾Ğ±ĞµĞ´ O: 4123, ĞĞ¸Ñ‡ÑŒĞ¸Ñ…/ĞŸÑƒÑÑ‚Ğ¾: 11125\n",
            "\n",
            "Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Epochs=150)...\n",
            "Epoch 1/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7362 - loss: 0.4879 - val_accuracy: 0.6806 - val_loss: 0.2782\n",
            "Epoch 2/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6871 - loss: 0.2552 - val_accuracy: 0.6852 - val_loss: 0.1772\n",
            "Epoch 3/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7050 - loss: 0.1675 - val_accuracy: 0.7265 - val_loss: 0.1318\n",
            "Epoch 4/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7238 - loss: 0.1219 - val_accuracy: 0.7430 - val_loss: 0.0985\n",
            "Epoch 5/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7226 - loss: 0.0870 - val_accuracy: 0.7492 - val_loss: 0.0709\n",
            "Epoch 6/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7286 - loss: 0.0656 - val_accuracy: 0.7332 - val_loss: 0.0526\n",
            "Epoch 7/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7231 - loss: 0.0452 - val_accuracy: 0.7539 - val_loss: 0.0390\n",
            "Epoch 8/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7226 - loss: 0.0322 - val_accuracy: 0.7451 - val_loss: 0.0261\n",
            "Epoch 9/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7304 - loss: 0.0224 - val_accuracy: 0.7632 - val_loss: 0.0207\n",
            "Epoch 10/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7302 - loss: 0.0164 - val_accuracy: 0.7234 - val_loss: 0.0139\n",
            "Epoch 11/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7240 - loss: 0.0107 - val_accuracy: 0.7420 - val_loss: 0.0109\n",
            "Epoch 12/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7226 - loss: 0.0079 - val_accuracy: 0.7095 - val_loss: 0.0075\n",
            "Epoch 13/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7098 - loss: 0.0058 - val_accuracy: 0.7353 - val_loss: 0.0060\n",
            "Epoch 14/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7147 - loss: 0.0044 - val_accuracy: 0.7214 - val_loss: 0.0044\n",
            "Epoch 15/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7051 - loss: 0.0032 - val_accuracy: 0.7229 - val_loss: 0.0035\n",
            "Epoch 16/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7102 - loss: 0.0026 - val_accuracy: 0.7105 - val_loss: 0.0028\n",
            "Epoch 17/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7034 - loss: 0.0021 - val_accuracy: 0.7234 - val_loss: 0.0027\n",
            "Epoch 18/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7092 - loss: 0.0016 - val_accuracy: 0.7141 - val_loss: 0.0019\n",
            "Epoch 19/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7013 - loss: 0.0013 - val_accuracy: 0.7193 - val_loss: 0.0016\n",
            "Epoch 20/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7006 - loss: 0.0011 - val_accuracy: 0.7126 - val_loss: 0.0014\n",
            "Epoch 21/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7009 - loss: 9.3615e-04 - val_accuracy: 0.7147 - val_loss: 0.0011\n",
            "Epoch 22/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7054 - loss: 8.3684e-04 - val_accuracy: 0.7136 - val_loss: 9.7753e-04\n",
            "Epoch 23/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7050 - loss: 7.2139e-04 - val_accuracy: 0.7152 - val_loss: 8.2073e-04\n",
            "Epoch 24/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7043 - loss: 5.8407e-04 - val_accuracy: 0.7152 - val_loss: 7.1729e-04\n",
            "Epoch 25/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7018 - loss: 5.1613e-04 - val_accuracy: 0.7147 - val_loss: 6.1623e-04\n",
            "Epoch 26/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7040 - loss: 4.3269e-04 - val_accuracy: 0.7136 - val_loss: 5.4291e-04\n",
            "Epoch 27/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7000 - loss: 3.8395e-04 - val_accuracy: 0.7136 - val_loss: 4.7181e-04\n",
            "Epoch 28/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7000 - loss: 3.4553e-04 - val_accuracy: 0.7157 - val_loss: 4.3196e-04\n",
            "Epoch 29/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6999 - loss: 2.9239e-04 - val_accuracy: 0.7141 - val_loss: 3.7330e-04\n",
            "Epoch 30/150\n",
            "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7020 - loss: 2.5195e-04 - val_accuracy: 0.7141 - val_loss: 3.2742e-04\n",
            "Epoch 31/150\n",
            "\u001b[1m 64/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7054 - loss: 2.3892e-04"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3635188631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# class_weight Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ±ĞµĞ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ğ½Ğ¸Ñ‡ÑŒĞ¸Ñ…\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# ĞĞ¾ Ñ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞµÑ‚ÑŒÑ Ğ¾Ğ½Ğ° ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑÑ Ğ¸ Ñ‚Ğ°Ğº.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# Ğ”Ğ°ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# --- 1. ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ GPU ---\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(f\"GPU Status: {'âœ… Active' if gpus else 'âš ï¸ Not found'}\")\n",
        "\n",
        "# --- 2. Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ¯ Ğ”ĞĞĞĞ«Ğ¥ ---\n",
        "def check_winner_class(board):\n",
        "    grid = np.array(board).reshape(3, 3)\n",
        "    win_x = False\n",
        "    win_o = False\n",
        "\n",
        "    lines = []\n",
        "    for i in range(3):\n",
        "        lines.append(grid[i, :])\n",
        "        lines.append(grid[:, i])\n",
        "    lines.append(grid.diagonal())\n",
        "    lines.append(np.fliplr(grid).diagonal())\n",
        "\n",
        "    for line in lines:\n",
        "        if np.all(line == 1): win_x = True\n",
        "        if np.all(line == 2): win_o = True\n",
        "\n",
        "    # ĞšĞ»Ğ°ÑÑÑ‹: 0 = ĞĞ¸Ñ‡ÑŒÑ/Ğ’ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ, 1 = ĞŸĞ¾Ğ±ĞµĞ´Ğ° X, 2 = ĞŸĞ¾Ğ±ĞµĞ´Ğ° O\n",
        "    if win_x and win_o: return -1 # ĞœÑƒÑĞ¾Ñ€ (Ğ¾Ğ±Ğ° Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ğ»Ğ¸)\n",
        "    if win_x: return 1\n",
        "    if win_o: return 2\n",
        "    return 0\n",
        "\n",
        "print(\"\\nĞ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...\")\n",
        "all_states = list(itertools.product([0, 1, 2], repeat=9))\n",
        "\n",
        "X_raw = []\n",
        "Y_raw = [] # Ğ¢ĞµĞ¿ĞµÑ€ÑŒ ÑÑ‚Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ¾ 0, 1 Ğ¸Ğ»Ğ¸ 2\n",
        "\n",
        "for state in all_states:\n",
        "    cls = check_winner_class(state)\n",
        "    if cls == -1: continue # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¼ÑƒÑĞ¾Ñ€\n",
        "\n",
        "    # Ğ’Ñ…Ğ¾Ğ´ 18 Ğ±Ğ¸Ñ‚\n",
        "    layer_x = [1 if cell == 1 else 0 for cell in state]\n",
        "    layer_o = [1 if cell == 2 else 0 for cell in state]\n",
        "\n",
        "    X_raw.append(layer_x + layer_o)\n",
        "    Y_raw.append(cls)\n",
        "\n",
        "X_data = np.array(X_raw)\n",
        "Y_data = np.array(Y_raw)\n",
        "\n",
        "# ĞŸĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°ĞµĞ¼\n",
        "indices = np.arange(len(X_data))\n",
        "np.random.shuffle(indices)\n",
        "X_data = X_data[indices]\n",
        "Y_data = Y_data[indices]\n",
        "\n",
        "# --- Ğ’Ğ«Ğ§Ğ˜Ğ¡Ğ›Ğ•ĞĞ˜Ğ• Ğ’Ğ•Ğ¡ĞĞ’ ĞšĞ›ĞĞ¡Ğ¡ĞĞ’ ---\n",
        "# Ğ­Ñ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğµ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğµ! ĞœÑ‹ ÑÑ‡Ğ¸Ñ‚Ğ°ĞµĞ¼, ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ· Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ,\n",
        "# Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ğ¼ ÑĞµÑ‚Ğ¸ ÑƒĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ´ĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ±ĞµĞ´Ğ°Ğ¼.\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(Y_data),\n",
        "    y=Y_data\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "print(f\"Ğ’ĞµÑĞ° ĞºĞ»Ğ°ÑÑĞ¾Ğ² (Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ): {class_weights_dict}\")\n",
        "# Ğ’Ñ‹ ÑƒĞ²Ğ¸Ğ´Ğ¸Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾ ĞºĞ»Ğ°ÑÑ 0 (ĞĞ¸Ñ‡ÑŒÑ) Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¼Ğ°Ğ»Ñ‹Ğ¹ Ğ²ĞµÑ, Ğ° 1 Ğ¸ 2 - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹.\n",
        "\n",
        "# --- 3. ĞĞ Ğ¥Ğ˜Ğ¢Ğ•ĞšĞ¢Ğ£Ğ Ğ (3 Ğ’Ğ«Ğ¥ĞĞ”Ğ) ---\n",
        "model = Sequential()\n",
        "# Ğ¡Ğ»Ğ¾Ğ¹ 1\n",
        "model.add(Dense(64, input_dim=18, activation='relu', name='L1'))\n",
        "# Ğ¡Ğ»Ğ¾Ğ¹ 2\n",
        "model.add(Dense(32, activation='relu', name='L2'))\n",
        "# Ğ’Ñ‹Ñ…Ğ¾Ğ´: 3 Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ° (Softmax Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¸Ğ· 3)\n",
        "model.add(Dense(3, activation='softmax', name='Output'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# --- 4. ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• ---\n",
        "print(\"\\nĞ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ...\")\n",
        "# ĞŸĞµÑ€ĞµĞ´Ğ°ĞµĞ¼ class_weight, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ\n",
        "history = model.fit(\n",
        "    X_data, Y_data,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    verbose=1,\n",
        "    class_weight=class_weights_dict, # ĞœĞĞ“Ğ˜Ğ¯ Ğ—Ğ”Ğ•Ğ¡Ğ¬\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "loss, acc = model.evaluate(X_data, Y_data, verbose=0)\n",
        "print(f\"\\nğŸ”¥ğŸ”¥ Ğ˜Ğ¢ĞĞ“ĞĞ’ĞĞ¯ Ğ¢ĞĞ§ĞĞĞ¡Ğ¢Ğ¬: {acc*100:.2f}% ğŸ”¥ğŸ”¥\")\n",
        "\n",
        "# --- 5. Ğ­ĞšĞ¡ĞŸĞĞ Ğ¢ Ğ’Ğ•Ğ¡ĞĞ’ ---\n",
        "if acc > 0.98:\n",
        "    SCALE = 128\n",
        "    def to_fixed(w_arr):\n",
        "        return [int(round(x * SCALE)) for x in w_arr.flatten()]\n",
        "\n",
        "    print(\"\\n--- COPY TO weights.txt ---\")\n",
        "\n",
        "    # L1\n",
        "    w, b = model.get_layer('L1').get_weights()\n",
        "    print(f\"localparam [15:0] L1_W [0:{len(to_fixed(w))-1}] = '{{ {', '.join(map(str, to_fixed(w)))} }};\")\n",
        "    print(f\"localparam [15:0] L1_B [0:{len(to_fixed(b))-1}] = '{{ {', '.join(map(str, to_fixed(b)))} }};\")\n",
        "\n",
        "    # L2\n",
        "    w, b = model.get_layer('L2').get_weights()\n",
        "    print(f\"localparam [15:0] L2_W [0:{len(to_fixed(w))-1}] = '{{ {', '.join(map(str, to_fixed(w)))} }};\")\n",
        "    print(f\"localparam [15:0] L2_B [0:{len(to_fixed(b))-1}] = '{{ {', '.join(map(str, to_fixed(b)))} }};\")\n",
        "\n",
        "    # Output (3 Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°)\n",
        "    w, b = model.get_layer('Output').get_weights()\n",
        "    print(f\"localparam [15:0] L3_W [0:{len(to_fixed(w))-1}] = '{{ {', '.join(map(str, to_fixed(w)))} }};\")\n",
        "    print(f\"localparam [15:0] L3_B [0:{len(to_fixed(b))-1}] = '{{ {', '.join(map(str, to_fixed(b)))} }};\")\n",
        "else:\n",
        "    print(\"Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¸Ğ·ĞºĞ°Ñ. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞµÑ‰Ğµ Ñ€Ğ°Ğ·.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFT2aee9Zfj2",
        "outputId": "b264df44-dbba-4ec3-8b0c-4b0374241e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Status: âœ… Active\n",
            "\n",
            "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...\n",
            "Ğ’ĞµÑĞ° ĞºĞ»Ğ°ÑÑĞ¾Ğ² (Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ): {0: np.float64(0.5804044943820225), 1: np.float64(1.5660926509822946), 2: np.float64(1.5660926509822946)}\n",
            "\n",
            "Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ...\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5008 - loss: 0.8423 - val_accuracy: 0.7848 - val_loss: 0.4872\n",
            "Epoch 2/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7944 - loss: 0.3982 - val_accuracy: 0.8555 - val_loss: 0.3386\n",
            "Epoch 3/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8555 - loss: 0.3022 - val_accuracy: 0.8720 - val_loss: 0.3091\n",
            "Epoch 4/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8912 - loss: 0.2446 - val_accuracy: 0.9190 - val_loss: 0.2248\n",
            "Epoch 5/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9187 - loss: 0.1924 - val_accuracy: 0.9231 - val_loss: 0.2010\n",
            "Epoch 6/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9486 - loss: 0.1391 - val_accuracy: 0.9639 - val_loss: 0.1283\n",
            "Epoch 7/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9648 - loss: 0.1091 - val_accuracy: 0.9757 - val_loss: 0.0998\n",
            "Epoch 8/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9744 - loss: 0.0839 - val_accuracy: 0.9819 - val_loss: 0.0745\n",
            "Epoch 9/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9793 - loss: 0.0676 - val_accuracy: 0.9856 - val_loss: 0.0585\n",
            "Epoch 10/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9815 - loss: 0.0509 - val_accuracy: 0.9897 - val_loss: 0.0451\n",
            "Epoch 11/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9857 - loss: 0.0416 - val_accuracy: 0.9892 - val_loss: 0.0383\n",
            "Epoch 12/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9876 - loss: 0.0336 - val_accuracy: 0.9917 - val_loss: 0.0325\n",
            "Epoch 13/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9921 - loss: 0.0244 - val_accuracy: 0.9948 - val_loss: 0.0249\n",
            "Epoch 14/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0199 - val_accuracy: 0.9964 - val_loss: 0.0159\n",
            "Epoch 15/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9959 - loss: 0.0144 - val_accuracy: 0.9985 - val_loss: 0.0127\n",
            "Epoch 16/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9979 - loss: 0.0104 - val_accuracy: 1.0000 - val_loss: 0.0085\n",
            "Epoch 17/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0077 - val_accuracy: 0.9995 - val_loss: 0.0076\n",
            "Epoch 18/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9995 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 0.0058\n",
            "Epoch 19/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0042\n",
            "Epoch 20/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0038\n",
            "Epoch 21/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0027\n",
            "Epoch 22/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 0.0022\n",
            "Epoch 23/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
            "Epoch 24/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
            "Epoch 25/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
            "Epoch 26/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 9.2756e-04 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
            "Epoch 27/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 7.6660e-04 - val_accuracy: 1.0000 - val_loss: 8.7329e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.0912e-04 - val_accuracy: 1.0000 - val_loss: 8.1245e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.9957e-04 - val_accuracy: 1.0000 - val_loss: 5.7660e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.1231e-04 - val_accuracy: 1.0000 - val_loss: 5.8190e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.5833e-04 - val_accuracy: 1.0000 - val_loss: 4.1025e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.9985e-04 - val_accuracy: 1.0000 - val_loss: 4.0171e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.5624e-04 - val_accuracy: 1.0000 - val_loss: 3.0771e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.1661e-04 - val_accuracy: 1.0000 - val_loss: 2.9787e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.9061e-04 - val_accuracy: 1.0000 - val_loss: 2.7534e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.4887e-04 - val_accuracy: 1.0000 - val_loss: 2.3516e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.2453e-04 - val_accuracy: 1.0000 - val_loss: 1.5373e-04\n",
            "Epoch 38/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.0191e-04 - val_accuracy: 1.0000 - val_loss: 1.9571e-04\n",
            "Epoch 39/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 8.8431e-05 - val_accuracy: 1.0000 - val_loss: 1.3494e-04\n",
            "Epoch 40/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 7.4851e-05 - val_accuracy: 1.0000 - val_loss: 1.0395e-04\n",
            "Epoch 41/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 6.3199e-05 - val_accuracy: 1.0000 - val_loss: 9.3394e-05\n",
            "Epoch 42/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 5.5429e-05 - val_accuracy: 1.0000 - val_loss: 7.3953e-05\n",
            "Epoch 43/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.4848e-05 - val_accuracy: 1.0000 - val_loss: 6.6846e-05\n",
            "Epoch 44/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 4.1485e-05 - val_accuracy: 1.0000 - val_loss: 5.2954e-05\n",
            "Epoch 45/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.3342e-05 - val_accuracy: 1.0000 - val_loss: 5.5309e-05\n",
            "Epoch 46/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.8064e-05 - val_accuracy: 1.0000 - val_loss: 4.4270e-05\n",
            "Epoch 47/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.4478e-05 - val_accuracy: 1.0000 - val_loss: 3.6618e-05\n",
            "Epoch 48/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.1284e-05 - val_accuracy: 1.0000 - val_loss: 2.6392e-05\n",
            "Epoch 49/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.7178e-05 - val_accuracy: 1.0000 - val_loss: 3.2808e-05\n",
            "Epoch 50/50\n",
            "\u001b[1m273/273\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.5585e-05 - val_accuracy: 1.0000 - val_loss: 2.8491e-05\n",
            "\n",
            "ğŸ”¥ğŸ”¥ Ğ˜Ğ¢ĞĞ“ĞĞ’ĞĞ¯ Ğ¢ĞĞ§ĞĞĞ¡Ğ¢Ğ¬: 100.00% ğŸ”¥ğŸ”¥\n",
            "\n",
            "--- COPY TO weights.txt ---\n",
            "localparam [15:0] L1_W [0:1151] = '{ 43, 0, -13, -3, 63, 4, 21, 39, 37, 29, -19, 63, 4, 10, 3, 12, 10, 3, 32, 9, 2, 44, 13, 7, -12, 35, -34, 7, 2, -135, -19, 7, -31, -5, -2, 8, 1, -12, -6, 4, 37, -145, -7, 10, 29, 75, 18, -72, -3, -152, -87, -12, -7, 11, -32, 32, -5, 0, -29, -53, 10, 79, -35, -8, 32, 30, -2, -3, 63, -4, 17, -44, 33, 19, -23, -17, 8, 4, 2, -1, -5, 4, -144, 0, -1, -2, 37, -2, -2, -14, -11, -101, 3, 77, 22, 25, -18, -7, -3, -10, -2, 22, 2, 73, -34, 21, -6, -9, 12, 22, 7, 18, 5, -52, -16, 0, -19, 12, 29, -50, 7, 8, 82, -16, 37, -21, -28, -12, 59, 19, -19, 0, -23, 32, 31, -50, 17, 35, 40, -31, -2, -25, 5, -6, 58, 2, 70, 0, -7, -35, 13, -20, 10, 30, -1, 16, -1, 41, -39, 4, -8, -1, -6, -10, -3, 30, 0, 47, -57, 20, 10, 29, -34, -6, -84, 38, -9, -62, -1, 0, 72, -47, 14, -37, -2, 2, -65, -59, -18, 84, 34, -10, 21, 41, 33, 3, 48, -9, 21, -114, -20, 9, 39, -10, 6, 34, 0, 0, -8, -29, -18, 22, -4, 3, 49, 17, -8, 16, 70, 4, 1, -28, 20, 28, 30, 6, 14, 19, 0, 4, 1, -5, 72, 54, 5, 27, -20, 23, -128, 10, 2, -13, 42, 3, -1, -13, -52, -6, 7, 0, -13, -27, 60, -32, 10, 1, 33, 31, 5, -6, 103, -12, 52, 18, -14, 38, 37, -16, 23, -21, 2, -4, 68, -7, 12, -14, 5, 4, 59, -12, 0, 16, 29, 0, -1, -39, -32, 57, 54, 9, 3, -3, 7, 14, -1, 101, -131, -62, 39, 23, 23, 124, -14, 5, 16, 67, 43, -53, 76, 12, -68, 24, 10, 0, 53, -40, -77, -39, -37, 2, 39, 1, 12, 20, -62, 2, 34, 29, 4, 24, 4, 49, 16, 40, -3, 1, 49, -11, -50, 0, 0, 4, 52, 13, -7, 5, -131, -13, 0, 31, -45, -5, 43, -10, 34, 2, 0, 14, -2, 5, -61, -33, 18, 17, -32, -47, 3, -67, 3, -18, 33, 5, -21, 68, 43, -23, 0, 6, -30, -48, -43, -2, -88, 2, 28, 38, -46, 0, 33, 21, -18, 5, -13, 7, 56, 29, -1, -29, 1, 0, 33, 10, 32, 11, -5, -3, 30, -72, 12, -13, -67, -56, 1, -15, 35, 13, 35, 11, 5, -4, 1, 40, 4, -38, 62, 40, 25, 26, -32, 3, 58, 57, -10, 6, -38, 1, -131, 4, 2, -11, -32, 9, -2, 66, -18, 73, -72, 57, -6, 22, 7, 13, 0, -6, -23, 10, -26, 25, 1, 16, 26, 59, 3, 7, 21, 5, -7, 5, 4, -2, 11, 0, 9, 60, 4, 48, 2, -21, 10, -3, 35, -8, -5, -3, 3, 43, 3, -62, -8, -1, 39, 36, -4, -26, -54, -52, 11, -8, -20, 0, -39, 21, -7, -8, 39, 5, -156, 71, -12, -17, 7, 7, 43, 35, -26, 7, -47, 29, -14, -96, -41, 57, 6, -48, 9, 45, 5, 40, -7, -5, -56, -2, 9, 1, 18, 4, -39, 32, -4, -43, -1, -19, 51, 43, 15, 0, 0, -7, 3, 24, 4, -4, -47, -42, 15, 40, 15, -94, -56, 7, -4, 82, 35, -65, -34, -55, 40, 27, -27, 1, -4, 80, 34, 48, 29, 18, 17, 33, 42, 37, -2, -42, -43, -12, 12, 0, 18, 6, 47, 26, 17, -5, 17, -115, -2, -45, -56, -40, -19, 39, 60, 10, 0, 10, -28, 29, 12, -7, 30, 23, -36, -25, -171, 21, 69, -1, -36, 5, 25, 35, 19, -11, 1, -3, 71, 4, 26, -118, -2, 2, 7, -8, -8, 13, 1, -3, -21, -1, 13, 27, 9, -3, 41, 36, -15, 77, -6, 4, 39, 62, 20, 16, -42, 12, -9, -26, -7, -48, 0, 30, -1, 81, 0, -9, 1, 28, -4, 5, -29, 10, 3, 48, 37, -13, -3, 57, 78, 37, -23, -6, 2, 4, 71, -28, 44, -8, 8, 16, -35, -1, -24, -21, -1, -22, 19, 16, 7, -147, -18, -3, 10, 21, -6, 27, -30, -38, -10, -41, 31, -98, -39, -6, 14, -28, 29, 0, 34, 5, -55, 71, 17, -27, -6, 34, 21, 50, -21, -40, 26, 26, 7, 9, -33, -22, 40, 33, 26, -23, 26, 69, 50, -11, -96, 3, 14, 6, 5, 9, 9, 25, 4, -26, 80, -2, 4, -9, -12, 45, 3, -14, -59, 24, 2, -2, 0, 15, 15, 32, 39, 18, 14, 47, -9, 12, -17, 11, 32, 42, 38, -4, 56, 22, -166, 19, -8, 72, -1, -94, 8, 6, 15, -105, -12, 30, -7, 6, 37, 12, 20, 33, 23, 28, 63, -37, 7, -1, 31, -3, -119, -1, -18, -43, 21, -9, -5, 4, -16, -9, -73, -39, -3, -9, 29, 9, -5, -22, -2, -6, -5, -3, 12, 20, -31, -15, 41, 61, -14, 45, -46, -1, 17, 62, -6, -10, -41, 45, 24, 13, 19, -21, 1, 101, -130, -23, 18, -64, -22, -6, -3, 1, 26, 3, 36, -53, 42, -113, 21, 22, -66, 27, -160, -7, 26, 16, -37, 26, 36, -16, 4, -29, -43, -6, -13, 53, -12, -13, 51, 70, -15, 18, -3, -4, -8, 8, -14, 48, 33, -13, -12, -59, -8, 37, 3, 8, 45, 50, 15, -3, 43, -4, 94, -70, -9, -58, 5, 86, 46, 29, -31, -18, -3, 31, 10, 3, 41, 1, 28, 71, 44, -1, -128, -14, 13, 6, 36, -2, 20, 3, 10, -29, 75, 8, 9, 17, -31, 3, 36, -9, -8, 6, -11, -1, -29, 6, -1, -1, 26, 16, 3, 45, -10, 42, 26, 46, -7, 35, 77, -7, 29, 21, 2, 19, 22, 25, 9, -28, 21, 26, 3, -29, 21, 22, 27, 61, -18, -9, -2, 1, 73, 4, 0, 10, -27, -109, -98, -12, -8, 38, -151, 3, -89, 0, 37, -36, 33, 31, 8, 5, 80, 3, 10, -22, 5, 1, 13, -17, 44, -48, 5, -12, -1, -2, 19, -23, 37, -48, 39, 36, -9, -32, 36, 9, 71, 33, 82, -12, -39, 23, -5, 0, 43, -3, -1, 19, 48, 54, -45, -38, 43, 45, 0, 6, -10, -1, -32, 28, 1, 50, -4, 39, -40, 24, 4, 14, 6, -5, -84, -5, 24, 6, -1, 27, -42, -4, -16, -15, 0, 11, 26, 34, -136, 69, 2, -22, 15, 8, 11, -26, -4, -30, -1, -30, 12, -18, 50, 6, -3, 9, 27, -6, 39, 11, -68, -20, 6, 12, 2, 33, -144, -42, -14, 1, -45, -24, 4, 6, 77, -11, -29, -48, -20, 26, 30, 73, -1, 2, 37, 9, 1, -5, 17, 7, 2, 50, 4, 32, 73, -12, -28, 55, -2, 28, 7, -14, 51, -67, 3, -39, 8, -1, -15, -30 };\n",
            "localparam [15:0] L1_B [0:63] = '{ -3, 21, -6, 2, 7, 17, 13, 23, 9, 4, 4, 49, -7, -23, 25, 39, -15, 8, 35, 8, 25, 11, -10, 14, 31, -1, 24, 21, -7, 2, 34, -14, -21, 1, 11, -4, 25, -18, 10, -8, -15, 26, 17, 26, 19, 22, 33, 24, 20, 0, 6, 1, -8, 30, 40, 29, 28, 31, 20, -61, 33, -23, 9, 0 };\n",
            "localparam [15:0] L2_W [0:2047] = '{ 29, -29, 20, 54, 10, 18, -9, 25, 10, 30, 12, -39, 41, 0, 13, -21, -28, 2, 25, 11, 16, 27, -6, -31, 34, 3, 37, 17, -13, 20, 20, 38, 23, 0, 51, -5, -17, -27, 34, 22, 46, 1, -19, 13, 11, 11, 10, 15, 16, 28, -31, -24, -17, 41, 53, 11, 48, 16, 23, -27, 5, 43, 4, 35, -10, -7, -35, 16, 61, 3, 31, -44, -2, -18, -42, -18, -43, 33, -17, -4, -2, 25, -8, 30, 36, -28, -18, 0, -8, 58, 62, 52, 6, -29, 19, -16, 1, -16, -1, 59, 79, 13, 60, -30, -10, -11, -48, -57, -4, -5, -27, 1, -17, -17, 42, 71, -12, 7, -23, -1, -3, 66, 28, 47, -29, -21, -17, -19, -24, -32, 60, 35, -22, -11, -31, 32, 43, 13, 51, 11, 44, -41, -10, -13, 11, 23, -22, -28, -12, -6, 36, -23, 72, 6, -49, -22, -68, 74, -53, 16, 8, -5, 61, -30, -75, 7, -59, 43, 48, 37, -15, 62, 25, 12, 79, 35, 35, -24, -66, -34, -14, 66, 80, 7, 37, -75, -21, -69, 24, 15, 41, -2, -34, 5, 64, -39, 27, -17, -88, 11, 52, 49, 15, 7, 31, -26, 64, -20, -41, 19, -30, -49, 30, 21, 70, -4, 11, -32, -26, -17, 26, 34, 38, 13, 148, 22, -32, -23, 56, -16, 9, -51, -18, 24, 62, 69, -49, 80, 8, 42, 7, 32, 2, 86, -83, 18, -29, 45, -50, 29, 61, 51, 31, -79, 3, -76, 22, -9, -30, 50, 27, -22, -2, 2, 11, 32, 34, 4, -23, -15, -29, -19, 2, 16, 55, 52, -62, -3, -31, 6, 20, 37, 1, 46, -3, -41, 13, -9, 18, 17, 32, 35, -18, 13, 21, 56, 39, 41, 8, -26, 0, 21, 8, -14, 16, -8, 35, 41, 11, -33, -6, -19, 12, 37, 40, 38, -16, 3, 36, 34, 20, -30, -8, 46, 27, 8, -20, 10, 25, -23, -12, 5, 7, -28, -12, 13, -10, 13, 35, 38, 3, 39, 16, -25, 28, 26, 11, 49, -4, 20, -24, 10, 38, -1, -29, -21, 74, 23, 14, -53, -42, 8, -33, 52, -34, 37, 35, 35, 41, 26, -36, 36, -83, 80, -40, 48, -18, -33, -15, -9, 42, -16, 27, -51, 10, 25, 25, 35, 27, -3, 55, -5, -32, -52, 8, -29, -11, -19, -82, -26, -10, -40, 58, 42, -1, 11, 20, -10, -9, 7, 22, 43, 9, -21, -25, 23, -74, -9, 0, 36, 6, -27, 12, 0, 11, -13, -37, -34, 31, 40, 22, -7, 1, 12, 23, -2, -2, -25, -3, -11, 11, 35, 16, 11, 9, 36, 19, -21, -30, -17, 126, -58, -6, -27, -81, 4, 106, 108, -37, 83, 31, 6, 160, 45, -5, 88, -97, -78, 4, 34, 113, 27, 33, -111, -110, -80, 39, 10, 97, 12, -27, 11, 98, -70, -35, -35, -41, 19, 80, 29, -75, 27, 40, 58, 62, 54, 4, 72, -48, -55, -32, 41, 91, -15, 8, -83, -92, -92, 61, -1, 4, 20, 9, -16, 15, -1, 15, -5, 18, 42, -4, -25, 28, 21, 41, -68, -18, -45, -70, 20, 25, -21, 3, -11, 27, -30, 42, 27, 0, 23, -7, 33, -42, 42, -18, -31, 66, -83, -81, -20, -110, 25, 49, 92, -49, 42, 18, 0, 120, 57, 22, 20, -50, -49, 11, 80, 94, 45, 33, -52, -91, -81, 41, -8, 43, 17, 105, 11, -66, 4, 19, 7, 53, -83, -51, 41, -2, 51, -68, 133, -2, 67, 64, 36, 23, 99, 18, 84, -66, 32, -87, 20, 41, 57, 51, -54, 66, -89, 19, -35, -61, 89, 38, -18, 63, -39, -8, 6, -38, -43, -19, -32, -16, -23, -14, -15, 53, 57, -66, -79, -41, -4, -3, 34, 88, 90, 7, -31, -38, -11, -43, -8, 84, -99, -34, 1, -116, 44, 130, 121, -37, 13, 54, -8, 178, 33, 20, 115, -104, -54, 51, 102, 67, 37, 46, -135, -126, -138, 58, 14, 72, 8, -25, 10, -54, 63, 55, -32, 39, 3, -40, -2, 6, -40, -30, -5, -26, 14, 30, -15, 41, 86, 23, -1, -76, 15, -18, 59, 69, 40, -29, -30, -7, -18, -34, 14, 0, 1, 41, -7, 2, 40, 20, 22, 39, -32, 51, -22, 14, -30, -7, -11, -12, -20, 37, -2, 51, 33, 2, -6, -34, -35, -42, 10, 12, 39, -123, 24, 59, -113, -87, -11, -79, 7, 38, 72, -63, -12, 2, 68, 91, 79, 76, 48, -113, -64, 43, 46, 118, 55, 1, -91, -119, -81, 81, -14, 35, -8, -8, -21, 55, -9, -6, -22, -12, 18, 42, 33, -21, 32, -3, 29, 52, 30, 1, 36, -34, -23, -13, 43, 24, 43, 22, 23, -7, 10, 42, 4, 26, -25, 6, -21, 0, 6, -5, 6, 18, 25, 48, -9, -53, -52, -14, 36, 13, -23, -15, 27, 28, 11, 21, -39, 11, -10, 39, 28, 16, -5, 21, -2, -2, 17, 76, 13, -56, -30, 95, -16, 17, -87, -40, 36, 67, 54, -54, 46, 0, 72, 99, 55, 5, 99, -64, 35, -60, 50, -85, -2, 17, 56, 26, -53, 27, -97, 129, -7, -47, -16, 54, -36, 47, -76, -39, 14, 11, 23, -90, 69, 11, 35, 27, 34, 2, 35, -84, 38, -60, 28, -80, 30, 6, 38, 27, -107, 21, -96, -13, -11, -19, 56, 23, 16, 31, 8, -63, -38, -59, -44, -25, -14, -93, -3, -2, -92, 21, 25, -13, -10, -32, -5, -32, 65, 41, 63, -24, -4, 11, -26, 78, 28, -13, -27, 60, 5, 29, -35, -12, -6, 120, 53, -58, -22, 55, 29, -6, 74, 0, 49, -60, 35, -20, 62, -22, 19, 26, 48, -1, -40, 32, -47, 3, 21, 1, 26, 9, -11, 4, -13, -34, -2, -4, 18, -50, 34, 18, 30, 64, 9, 3, 65, -15, 8, -8, 13, -34, 10, 39, 64, -9, -48, 20, -50, 17, -32, -20, 22, -8, 25, 54, 21, -13, 5, 30, -27, 48, -30, -17, 13, 33, 9, 47, 25, 37, -44, -32, -7, 14, 5, 51, 29, -47, 38, 26, 35, -20, -1, -7, 35, 4, 17, 46, 23, 3, 8, -31, 1, 42, -58, -6, -44, -43, -2, 53, -7, 51, -48, 31, 27, 22, 18, 37, 31, -34, 44, 4, 40, -61, -27, 77, -53, -48, -12, -60, 51, 87, 43, 1, 29, 20, 42, 108, 11, 25, 40, -50, -57, 4, 64, 65, 26, 19, -74, -84, -37, 15, -16, 58, 9, -52, -5, 79, -67, -43, 2, -23, 29, 73, 74, 18, 14, 12, 31, 127, 47, 1, 49, -12, -35, 29, 64, 91, 33, 42, -40, -56, -67, 48, 35, -44, 19, 23, -13, -13, 22, 41, 20, 52, -14, -63, -54, -4, -51, -12, 44, -28, -19, 27, -8, 62, 41, 24, 17, -13, -15, -30, 25, 75, 55, -9, -31, -90, 16, -61, -27, 124, -88, -29, -22, -106, 15, 146, 113, 23, 43, 40, 38, 153, 94, 3, 89, -87, -94, 48, 53, 112, 27, 24, -181, -98, -131, 106, 30, 70, -2, -26, -19, 18, 24, 8, 19, 34, 28, 24, -3, -46, -5, 35, -12, 1, -20, -22, -34, 51, 0, -23, -30, 27, -24, 7, -1, 25, 4, 3, -6, -27, 35, -100, -20, 90, -78, -50, -18, -113, 44, 77, 137, 0, 47, 3, 20, 185, 82, 30, 97, -68, -56, 44, 75, 132, 21, 37, -171, -134, -105, 70, 4, 93, 12, -24, 0, 32, 34, 14, -34, -61, 30, 47, 3, 50, -17, 59, -81, -1, 11, -31, -9, -10, -48, 39, -23, 41, -31, 62, 13, 14, -5, -75, 55, -50, 48, -83, -14, 65, -32, -48, 21, -33, 100, 77, 45, -35, -20, 107, -121, 52, -53, -75, -24, -66, -81, -1, -123, 106, -40, 142, -77, -53, -94, -26, 105, 14, 81, 74, 17, -9, -38, 123, -31, 38, -77, -40, -16, 21, 46, -106, 38, 25, 60, 86, 15, 16, 91, 19, 19, -17, 93, -103, 14, 26, 19, 26, -94, 38, -107, 9, 24, 54, -35, -18, -36, -25, 60, 6, 53, -23, 34, 60, -32, 49, -19, -35, -26, -55, -22, 5, 38, 65, 23, 47, -57, -67, -38, 44, 38, 34, 25, -18, -31, 31, 1, 7, -28, -25, 39, 46, 21, -44, 3, 3, 20, 50, 34, 15, 31, 5, -20, 29, 6, 50, 1, 29, -46, -46, -26, -4, 48, 18, 18, -11, 3, -16, 22, 13, -29, 46, -27, -10, 30, 22, 25, 8, 31, -28, 8, 12, -15, 24, 43, 7, -25, 3, 22, -12, 10, 34, 39, 20, -31, 33, 7, 8, 24, 72, -16, -1, -6, -84, 65, 24, -25, 63, 3, 51, -9, 32, 1, -62, -43, 5, -53, 9, -26, 75, -6, 50, -34, -42, 8, -19, 13, -31, 49, 204, 17, -39, -25, 54, 21, 41, -131, -25, 11, 86, 47, -120, 30, 20, 37, 81, 35, 30, 99, -5, 34, -31, 40, -141, 10, 69, 36, 62, -132, 19, -117, 37, -34, -28, 41, 24, -18, 16, -12, -6, 11, 50, 19, -5, 12, -22, 21, 61, 17, 26, 34, -12, 28, -30, 35, -36, 0, 31, 58, -16, -26, 38, -41, 19, -25, -36, 33, 16, 5, 50, -37, -87, -141, -10, 18, -40, 4, -103, -18, 28, -128, 14, 69, 20, -12, -58, 12, -17, 52, 60, 58, -23, -26, -86, -19, 185, 24, -85, -6, 109, 29, 55, -124, -76, -3, 111, 87, -74, 102, -9, 73, 96, 85, 17, 76, -96, 85, -98, 109, -97, 7, 29, 45, 54, -93, 29, -117, 7, -3, 53, -1, -33, -7, -30, 50, 69, 10, 7, 22, 75, -44, 33, -1, -5, -19, -42, -24, 69, -36, 93, -8, 43, -1, -34, -32, -27, 70, 31, 31, -57, 23, 83, -78, -36, -32, -82, 25, 58, 142, -14, 31, -41, -8, 144, 80, 31, 97, -100, -11, -23, 66, 71, 85, 3, -102, -103, -57, 104, -33, 112, -9, 156, 1, -75, -17, 76, 15, 8, -100, -81, 4, 120, 91, -135, 80, -19, 88, 29, 47, -21, 137, -68, 73, -111, 94, -140, 4, 44, 86, 24, -134, 56, -71, 45, -2, -13, -5, 44, -6, -33, -12, -30, -28, 2, 43, -52, 53, -19, 37, 47, 29, -37, 18, -30, 40, 14, 63, -15, -35, 34, -4, 36, -46, -12, -60, 5, -19, -6, 36, 33, -31, 28, 11, 31, 27, 42, 29, -1, -6, -2, 30, 27, 27, 11, 53, -135, 30, -7, 52, -11, 14, 26, 48, 28, -22, -20, -47, 48, -28, 15, -32, -19, -18, 15, 1, -6, 55, -71, 43, -51, 34, 53, 37, 54, 43, -15, 29, 52, 11, 32, 23, -16, -13, 34, 7, 30, -19, 19, -47, -3, -25, 51, -43, -44, 21, -17, 52, 9, 85, -74, 33, 13, 82, 24, 54, 57, -25, -72, -5, -4, 48, 75, 32, 36, -68, -79, -38, 45, 5, 47, -20, -45, 27, 118, -92, -72, -20, -87, 49, 92, 116, -21, 20, 41, 7, 148, 44, 34, 41, -103, -61, 29, 8, 139, 44, 14, -130, -73, -62, 69, 6, 94, 20, 150, -12, -22, -52, 107, 1, 57, -91, -33, 0, 119, 61, -104, 49, 44, 70, 94, 24, -16, 108, -44, 32, -56, 91, -62, 23, 11, 40, 1, -97, 42, -64, -104, -15, 97, 88, -167, 22, -121, 140, 130, -5, -56, -83, 103, -127, 58, -72, -161, -42, 23, -126, -18, -99, 131, -106, 143, -45, -86, -107, -91, 121, -70, 122, -24, -30, -10, 14, 30, -31, 39, -48, -25, 2, 0, -24, -57, 62, 4, 32, 72, 48, 16, 62, -58, 55, -20, 60, -12, -22, 36, 8, 45, -8, 11, -62, 44, 25, 45, 17, -139, -26, 33, 67, 15, -24, -102, 15, 64, 8, 2, -49, -33, -38, -15, -29, 11, -46, 17, -110, 47, 29, 36, -2, -20, 65, -9, 29, 33, 21, -2, -19, 39, -7, 46, -56, -36, -10, -23, 48, -39, 31, 12, -13, 39, 2, 35, 72, -46, 47, -44, 47, -66, -22, 35, 0, 10, -84, -5, -33, 56, 0, -6, 8, 42, -1, 63, -13, 16, 20, -77, 21, -13, -14, -16, -30, 0, -21, 44, 23, -2, -25, -26, -34, 6, 43, 12, 60, -24, 15, -3, 2 };\n",
            "localparam [15:0] L2_B [0:31] = '{ 11, -3, 20, -13, 11, -3, 6, -5, 18, 28, -15, 20, -10, 37, 27, 26, 24, 28, -9, 21, -6, 29, 11, 28, -2, -6, 10, 9, 29, -18, 24, -21 };\n",
            "localparam [15:0] L3_W [0:95] = '{ 67, -247, 144, -22, -46, 42, 27, 109, -153, -79, 41, 140, 36, -135, 115, -45, -4, 13, -50, -95, 171, -108, 101, -147, 17, 109, -163, 106, 36, -220, 119, -230, 91, 156, -132, -98, -129, 150, -113, 150, -149, 31, 71, 44, -168, 130, -113, -82, 89, -202, 5, 149, -53, -205, -113, -30, 136, 10, -112, 104, -218, 195, -48, 137, -122, -149, 45, 119, -108, 143, -191, -20, -96, 113, -113, -97, -1, 187, -72, -98, 119, -53, -102, 111, 165, -110, -121, -114, 126, -44, 147, -45, -170, -152, 162, 8 };\n",
            "localparam [15:0] L3_B [0:2] = '{ 21, -20, -1 };\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ¯ TESTBENCH (3 ĞšĞ›ĞĞ¡Ğ¡Ğ) ---\")\n",
        "# Ğ­Ñ‚Ğ¾Ñ‚ ĞºĞ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ´Ğ»Ñ Verilog\n",
        "\n",
        "# Ğ’Ñ‹Ğ±ĞµÑ€ĞµĞ¼ 5 ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² + ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ\n",
        "test_indices = [0, 1, 10, 50, 100]\n",
        "\n",
        "print(\"// Ğ’ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ ÑÑ‚Ğ¾Ñ‚ ĞºĞ¾Ğ´ Ğ² initial block Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Testbench.sv\")\n",
        "print(\"initial begin\")\n",
        "print(\"    // Reset sequence\")\n",
        "print(\"    rst = 1; start = 0; #20; rst = 0; #20;\")\n",
        "\n",
        "for i in test_indices:\n",
        "    in_vec = X_data[i]\n",
        "    y_true = Y_data[i] # 0, 1 Ğ¸Ğ»Ğ¸ 2\n",
        "\n",
        "    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ÑÑ‚Ñ€Ğ¾ĞºÑƒ Ğ±Ğ¸Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Verilog (18 Ğ±Ğ¸Ñ‚)\n",
        "    # X_data Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ int 0/1, ÑĞºĞ»ĞµĞ¸Ğ²Ğ°ĞµĞ¼ Ğ² ÑÑ‚Ñ€Ğ¾ĞºÑƒ\n",
        "    bin_str = \"\".join(map(str, map(int, in_vec)))\n",
        "\n",
        "    print(f\"\\n    // Test Case {i}: ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑ {y_true} (0=Draw, 1=X, 2=O)\")\n",
        "\n",
        "    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼ Ğ½Ğ° X Ğ¸ O Ğ´Ğ»Ñ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ²Ğ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ (ĞºĞ°Ğº Ğ¼Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ Ğ±ÑƒÑ„ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹)\n",
        "    # ĞŸĞµÑ€Ğ²Ñ‹Ğµ 9 Ğ±Ğ¸Ñ‚ - ĞºÑ€ĞµÑÑ‚Ğ¸ĞºĞ¸, Ğ²Ñ‚Ğ¾Ñ€Ñ‹Ğµ 9 Ğ±Ğ¸Ñ‚ - Ğ½Ğ¾Ğ»Ğ¸ĞºĞ¸\n",
        "    print(f\"    // Loading X inputs\")\n",
        "    print(f\"    switches = 9'b{bin_str[0:9]};\")\n",
        "    print(f\"    btn_load_x = 1; #10; btn_load_x = 0; #10;\")\n",
        "\n",
        "    print(f\"    // Loading O inputs\")\n",
        "    print(f\"    switches = 9'b{bin_str[9:18]};\")\n",
        "    print(f\"    btn_load_o = 1; #10; btn_load_o = 0; #10;\")\n",
        "\n",
        "    print(f\"    start = 1; #100; start = 0; // Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ\")\n",
        "\n",
        "    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°\n",
        "    # Ğ’ Verilog Ñƒ Ğ½Ğ°Ñ Ğ±ÑƒĞ´ĞµÑ‚ 3 Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°: win_x, win_o, draw\n",
        "    if y_true == 1:\n",
        "        print(f\"    if (led_win_x !== 1) $display(\\\"ERROR Case {i}: Expected X Win\\\");\")\n",
        "        print(f\"    else $display(\\\"PASS Case {i}: X Win Correct\\\");\")\n",
        "    elif y_true == 2:\n",
        "        print(f\"    if (led_win_o !== 1) $display(\\\"ERROR Case {i}: Expected O Win\\\");\")\n",
        "        print(f\"    else $display(\\\"PASS Case {i}: O Win Correct\\\");\")\n",
        "    else:\n",
        "        print(f\"    if (led_draw !== 1) $display(\\\"ERROR Case {i}: Expected Draw\\\");\")\n",
        "        print(f\"    else $display(\\\"PASS Case {i}: Draw Correct\\\");\")\n",
        "\n",
        "print(\"\\n    $finish;\")\n",
        "print(\"end\")"
      ],
      "metadata": {
        "id": "4YN_vUX5aI1g",
        "outputId": "963e2a94-72ce-4a0f-81eb-94dc10bc2e22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ˜Ğ¯ TESTBENCH (3 ĞšĞ›ĞĞ¡Ğ¡Ğ) ---\n",
            "// Ğ’ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ ÑÑ‚Ğ¾Ñ‚ ĞºĞ¾Ğ´ Ğ² initial block Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Testbench.sv\n",
            "initial begin\n",
            "    // Reset sequence\n",
            "    rst = 1; start = 0; #20; rst = 0; #20;\n",
            "\n",
            "    // Test Case 0: ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑ 2 (0=Draw, 1=X, 2=O)\n",
            "    // Loading X inputs\n",
            "    switches = 9'b010000001;\n",
            "    btn_load_x = 1; #10; btn_load_x = 0; #10;\n",
            "    // Loading O inputs\n",
            "    switches = 9'b101111010;\n",
            "    btn_load_o = 1; #10; btn_load_o = 0; #10;\n",
            "    start = 1; #100; start = 0; // Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ\n",
            "    if (led_win_o !== 1) $display(\"ERROR Case 0: Expected O Win\");\n",
            "    else $display(\"PASS Case 0: O Win Correct\");\n",
            "\n",
            "    // Test Case 1: ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑ 1 (0=Draw, 1=X, 2=O)\n",
            "    // Loading X inputs\n",
            "    switches = 9'b000001111;\n",
            "    btn_load_x = 1; #10; btn_load_x = 0; #10;\n",
            "    // Loading O inputs\n",
            "    switches = 9'b100010000;\n",
            "    btn_load_o = 1; #10; btn_load_o = 0; #10;\n",
            "    start = 1; #100; start = 0; // Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ\n",
            "    if (led_win_x !== 1) $display(\"ERROR Case 1: Expected X Win\");\n",
            "    else $display(\"PASS Case 1: X Win Correct\");\n",
            "\n",
            "    // Test Case 10: ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑ 2 (0=Draw, 1=X, 2=O)\n",
            "    // Loading X inputs\n",
            "    switches = 9'b000000010;\n",
            "    btn_load_x = 1; #10; btn_load_x = 0; #10;\n",
            "    // Loading O inputs\n",
            "    switches = 9'b001001001;\n",
            "    btn_load_o = 1; #10; btn_load_o = 0; #10;\n",
            "    start = 1; #100; start = 0; // Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ\n",
            "    if (led_win_o !== 1) $display(\"ERROR Case 10: Expected O Win\");\n",
            "    else $display(\"PASS Case 10: O Win Correct\");\n",
            "\n",
            "    // Test Case 50: ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑ 2 (0=Draw, 1=X, 2=O)\n",
            "    // Loading X inputs\n",
            "    switches = 9'b000100110;\n",
            "    btn_load_x = 1; #10; btn_load_x = 0; #10;\n",
            "    // Loading O inputs\n",
            "    switches = 9'b111000001;\n",
            "    btn_load_o = 1; #10; btn_load_o = 0; #10;\n",
            "    start = 1; #100; start = 0; // Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ\n",
            "    if (led_win_o !== 1) $display(\"ERROR Case 50: Expected O Win\");\n",
            "    else $display(\"PASS Case 50: O Win Correct\");\n",
            "\n",
            "    // Test Case 100: ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼ ĞºĞ»Ğ°ÑÑ 1 (0=Draw, 1=X, 2=O)\n",
            "    // Loading X inputs\n",
            "    switches = 9'b011001011;\n",
            "    btn_load_x = 1; #10; btn_load_x = 0; #10;\n",
            "    // Loading O inputs\n",
            "    switches = 9'b000110100;\n",
            "    btn_load_o = 1; #10; btn_load_o = 0; #10;\n",
            "    start = 1; #100; start = 0; // Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ\n",
            "    if (led_win_x !== 1) $display(\"ERROR Case 100: Expected X Win\");\n",
            "    else $display(\"PASS Case 100: X Win Correct\");\n",
            "\n",
            "    $finish;\n",
            "end\n"
          ]
        }
      ]
    }
  ]
}